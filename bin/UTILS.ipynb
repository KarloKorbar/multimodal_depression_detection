{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Parsing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e987fc34b9e63179"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "base_directory = 'data'\n",
    "\n",
    "all_files = [\n",
    "    'TRANSCRIPT.csv',\n",
    "    'AUDIO.wav',\n",
    "    'FORMANT.csv',\n",
    "    'COVAREP.csv',\n",
    "    'CLNF_gaze.txt',\n",
    "    'CLNF_AUs.txt',\n",
    "    'CLNF_hog.bin',\n",
    "    'CLNF_features.txt',\n",
    "    'CLNF_pose.txt',\n",
    "    'CLNF_features3D.txt'\n",
    "]\n",
    "text_files = [\n",
    "    'TRANSCRIPT.csv',\n",
    "]\n",
    "audio_files = [\n",
    "    'AUDIO.wav',\n",
    "    'FORMANT.csv',\n",
    "    'COVAREP.csv'\n",
    "]\n",
    "face_files = [\n",
    "    'CLNF_gaze.txt',\n",
    "    'CLNF_AUs.txt',\n",
    "    'CLNF_hog.bin',\n",
    "    'CLNF_features.txt',\n",
    "    'CLNF_pose.txt',\n",
    "    'CLNF_features3D.txt'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T17:08:36.801966Z",
     "start_time": "2024-11-02T17:08:36.783829Z"
    }
   },
   "id": "b7e4fe5988d79d8f",
   "execution_count": 139
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to retrieve the necessary files\n",
    "def retrieve_files(base_dir = base_directory, required_files=[], folder_ids=None):\n",
    "    # Create a list to store the data for the DataFrame\n",
    "    data = []\n",
    "\n",
    "    # Get the list of subfolders\n",
    "    subfolders = [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))]\n",
    "\n",
    "    # If folder_ids is specified, filter the subfolders list based on the provided IDs\n",
    "    if folder_ids:\n",
    "        subfolders = [f for f in subfolders if int(f.split('_')[0]) in folder_ids]\n",
    "\n",
    "    # Iterate through each (filtered) subfolder in the base directory\n",
    "    for subfolder in subfolders:\n",
    "        subfolder_path = os.path.join(base_dir, subfolder)\n",
    "\n",
    "        # Create a dictionary to store the file paths for the current subfolder\n",
    "        formatted_subfolder = subfolder.split('_')[0]  # getting rid of _P\n",
    "        subfolder_files = {'ID': formatted_subfolder}\n",
    "\n",
    "        # Iterate through each required file\n",
    "        for file_name in required_files:\n",
    "            file_path = os.path.join(subfolder_path, f\"{subfolder[:3]}_{file_name}\")\n",
    "\n",
    "            # Check if the file exists and add it to the dictionary\n",
    "            formatted_file_name = file_name.split('.')[0]\n",
    "            subfolder_files[formatted_file_name] = file_path if os.path.exists(file_path) else None\n",
    "\n",
    "        # Append the dictionary to the data list\n",
    "        data.append(subfolder_files)\n",
    "\n",
    "    # Create a DataFrame from the data list\n",
    "    df = pd.DataFrame(data)\n",
    "    df['ID'] = df['ID'].astype(int)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T17:08:36.802860Z",
     "start_time": "2024-11-02T17:08:36.789692Z"
    }
   },
   "id": "ef9d18f0f646bbac",
   "execution_count": 140
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "phq_paths = [\n",
    "    'testing/dev_split_Depression_AVEC2017.csv',\n",
    "    'testing/full_test_split.csv',\n",
    "    'testing/train_split_Depression_AVEC2017.csv'\n",
    "]\n",
    "\n",
    "def append_PHQ_Binary(df):\n",
    "    phq_dataframes = []\n",
    "\n",
    "    for path in phq_paths:\n",
    "        phq_df = pd.read_csv(path)\n",
    "        \n",
    "        if 'PHQ_Binary' in phq_df.columns:\n",
    "            phq_column = 'PHQ_Binary'\n",
    "        elif 'PHQ8_Binary' in phq_df.columns:\n",
    "            phq_column = 'PHQ8_Binary'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        phq_df = phq_df[['Participant_ID', phq_column]]\n",
    "        phq_df.rename(columns={phq_column: 'PHQ_Binary'}, inplace=True)\n",
    "        phq_dataframes.append(phq_df)\n",
    "\n",
    "    combined_phq_df = pd.concat(phq_dataframes, ignore_index=True)\n",
    "    df = df.merge(combined_phq_df, how='left', left_on='ID', right_on='Participant_ID')\n",
    "\n",
    "    df.drop(columns=['Participant_ID'], inplace=True)\n",
    "    df.dropna(subset=['PHQ_Binary'], inplace=True)\n",
    "    return df\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T17:08:36.803312Z",
     "start_time": "2024-11-02T17:08:36.794624Z"
    }
   },
   "id": "2df620f3a2838f98",
   "execution_count": 141
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_balanced_subset(df, percentage, random_state):\n",
    "    # Calculate the desired size of the subset\n",
    "    target_size = int(len(df) * percentage)\n",
    "    \n",
    "    # Split the dataframe by PHQ_Binary values\n",
    "    df_0 = df[df['PHQ_Binary'] == 0]\n",
    "    df_1 = df[df['PHQ_Binary'] == 1]\n",
    "    \n",
    "    # Determine the maximum number of samples for each PHQ_Binary group\n",
    "    max_samples_per_class = min(len(df_0), len(df_1), target_size // 2)\n",
    "    \n",
    "    # Sample from each group\n",
    "    sampled_0 = df_0.sample(n=max_samples_per_class, random_state=random_state)\n",
    "    sampled_1 = df_1.sample(n=max_samples_per_class, random_state=random_state)\n",
    "    \n",
    "    # Combine the samples and shuffle\n",
    "    balanced_subset = pd.concat([sampled_0, sampled_1]).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return balanced_subset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T17:08:36.813665Z",
     "start_time": "2024-11-02T17:08:36.806420Z"
    }
   },
   "id": "6db2cec89928dc48",
   "execution_count": 142
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import util.pp_text as pp_text\n",
    "import util.pp_audio as pp_audio\n",
    "import util.pp_face as pp_face\n",
    "\n",
    "def get_feature_subset_df(base_dir=base_directory, required_files=all_files, folder_ids=None):\n",
    "    path_map = retrieve_files(base_dir, required_files, folder_ids)\n",
    "    df = pd.DataFrame()\n",
    "    feature_map = {\n",
    "        'TRANSCRIPT': (pp_text.preprocess_TRANSCRIPT, 'TRANSCRIPT_'),\n",
    "        'AUDIO': (pp_audio.preprocess_AUDIO, 'AUDIO_'),\n",
    "        'FORMANT': (pp_audio.preprocess_FORMANT, 'FORMANT_'),\n",
    "        'COVAREP': (pp_audio.preprocess_COVAREP, 'COVAREP_'),\n",
    "        'CLNF_gaze': (pp_face.preprocess_CLNF_gaze, 'CLNFgaze_'),\n",
    "        'CLNF_AUs': (pp_face.preprocess_CLNF_AUs, 'CLNFAUs_'),\n",
    "        'CLNF_hog': (pp_face.preprocess_CLNF_hog, 'CLNFhog_'),\n",
    "        'CLNF_features': (pp_face.preprocess_CLNF_features, 'CLNFfeatures_'),\n",
    "        'CLNF_pose': (pp_face.preprocess_CLNF_pose, 'CLNFpose_'),\n",
    "        'CLNF_features3D': (pp_face.preprocess_CLNF_features3D, 'CLNFfeatures3D_')\n",
    "    }\n",
    "    \n",
    "    for i in path_map.index:\n",
    "        df_concat = pd.DataFrame()\n",
    "        # Loop through feature_map and process if column exists in pathMap\n",
    "        for column, (preprocess_func, prefix) in feature_map.items():\n",
    "            if column in path_map.columns:\n",
    "                processed_feature = preprocess_func(path_map[column][i]).add_prefix(prefix)\n",
    "                df_concat = pd.concat([df_concat, processed_feature], axis=1)\n",
    "        # Add ID\n",
    "        df_concat['ID'] = path_map['ID'].iloc[i]\n",
    "        # Add PHQ binary   \n",
    "        # df_concat = append_PHQ_Binary(df_concat) \n",
    "        \n",
    "        # Append the concatenated dataframe for each index to the main df\n",
    "        df = pd.concat([df, df_concat], ignore_index=True)\n",
    "        \n",
    "    # Format column names properly\n",
    "    df.columns = df.columns.str.replace(r'[^\\w]', '', regex=True)\n",
    "\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T17:08:36.814406Z",
     "start_time": "2024-11-02T17:08:36.811926Z"
    }
   },
   "id": "95e5a065b0c6f6a2",
   "execution_count": 143
  },
  {
   "cell_type": "markdown",
   "source": [
    "# testing method above:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc13c4aa5ae8ad60"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_subset(percentage, random_state):\n",
    "    df_all = retrieve_files()\n",
    "    df_all = append_PHQ_Binary(df_all)\n",
    "    df_subset = get_balanced_subset(df_all, percentage, random_state)\n",
    "    subset = df_subset['ID'].to_list()\n",
    "    return subset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T17:08:36.820381Z",
     "start_time": "2024-11-02T17:08:36.814534Z"
    }
   },
   "id": "903f8bdc210271d1",
   "execution_count": 144
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     PHQ_Binary\nID             \n386           1\n388           1\n421           1\n476           0\n413           1\n366           0\n322           0\n391           0\n353           1\n443           0\n452           0\n384           1\n418           1\n306           0\n337           1\n305           0\n330           1\n489           0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PHQ_Binary</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>386</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>388</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>421</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>476</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>413</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>366</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>322</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>391</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>353</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>443</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>452</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>384</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>418</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>306</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>337</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>305</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>330</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>489</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_id(percentage = 0, random_state = 42):\n",
    "    df = retrieve_files(folder_ids=get_subset(percentage, random_state))\n",
    "    df = append_PHQ_Binary(df)\n",
    "    df.set_index('ID', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "get_id(0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T17:14:55.597942Z",
     "start_time": "2024-11-02T17:14:55.571630Z"
    }
   },
   "id": "3109c3a4699413bb",
   "execution_count": 150
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                       TRANSCRIPT_text\nID                                                    \n386  synch introv4confirmation hi im ellie thanks f...\n388  sync introv4confirmation hi im ellie thanks fo...\n421  synch introv4confirmation hi im ellie thanks f...\n476  synch introv4confirmation hi im ellie thanks f...\n413  sync introv4confirmation hi im ellie thanks fo...\n366  synch introv4confirmation hi im ellie thanks f...\n322  hi im ellie thanks for coming in today i was c...\n391  sync introv4confirmation hi im ellie thanks fo...\n353  hi im ellie thanks for coming in today i was c...\n443  synch introv4confirmation hi im ellie thanks f...\n452  sync introv4confirmation hi im ellie thanks fo...\n384  sync introv4confirmation hi im ellie thanks fo...\n418  sync introv4confirmation hi im ellie thanks fo...\n306  hi im ellie thanks for coming in today i was c...\n337  hi im ellie thanks for coming in today i was c...\n305  hi im ellie thanks for coming in today i was c...\n330  laughter hi im ellie thanks for coming in toda...\n489  synch introv4confirmation hi im ellie thanks f...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TRANSCRIPT_text</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>386</th>\n      <td>synch introv4confirmation hi im ellie thanks f...</td>\n    </tr>\n    <tr>\n      <th>388</th>\n      <td>sync introv4confirmation hi im ellie thanks fo...</td>\n    </tr>\n    <tr>\n      <th>421</th>\n      <td>synch introv4confirmation hi im ellie thanks f...</td>\n    </tr>\n    <tr>\n      <th>476</th>\n      <td>synch introv4confirmation hi im ellie thanks f...</td>\n    </tr>\n    <tr>\n      <th>413</th>\n      <td>sync introv4confirmation hi im ellie thanks fo...</td>\n    </tr>\n    <tr>\n      <th>366</th>\n      <td>synch introv4confirmation hi im ellie thanks f...</td>\n    </tr>\n    <tr>\n      <th>322</th>\n      <td>hi im ellie thanks for coming in today i was c...</td>\n    </tr>\n    <tr>\n      <th>391</th>\n      <td>sync introv4confirmation hi im ellie thanks fo...</td>\n    </tr>\n    <tr>\n      <th>353</th>\n      <td>hi im ellie thanks for coming in today i was c...</td>\n    </tr>\n    <tr>\n      <th>443</th>\n      <td>synch introv4confirmation hi im ellie thanks f...</td>\n    </tr>\n    <tr>\n      <th>452</th>\n      <td>sync introv4confirmation hi im ellie thanks fo...</td>\n    </tr>\n    <tr>\n      <th>384</th>\n      <td>sync introv4confirmation hi im ellie thanks fo...</td>\n    </tr>\n    <tr>\n      <th>418</th>\n      <td>sync introv4confirmation hi im ellie thanks fo...</td>\n    </tr>\n    <tr>\n      <th>306</th>\n      <td>hi im ellie thanks for coming in today i was c...</td>\n    </tr>\n    <tr>\n      <th>337</th>\n      <td>hi im ellie thanks for coming in today i was c...</td>\n    </tr>\n    <tr>\n      <th>305</th>\n      <td>hi im ellie thanks for coming in today i was c...</td>\n    </tr>\n    <tr>\n      <th>330</th>\n      <td>laughter hi im ellie thanks for coming in toda...</td>\n    </tr>\n    <tr>\n      <th>489</th>\n      <td>synch introv4confirmation hi im ellie thanks f...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text(percentage = 0, random_state = 42):\n",
    "    text_test = [\n",
    "        'TRANSCRIPT.csv',\n",
    "    ]\n",
    "    \n",
    "    df = get_feature_subset_df(required_files=text_test, folder_ids=get_subset(percentage, random_state))\n",
    "    df.set_index('ID', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "get_text(0.1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T17:08:36.874247Z",
     "start_time": "2024-11-02T17:08:36.830216Z"
    }
   },
   "id": "b312c87381627581",
   "execution_count": 146
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_audio(percentage = 0, random_state = 42):\n",
    "    audio_test = [\n",
    "        #'AUDIO.wav',\n",
    "        'FORMANT.csv',\n",
    "        'COVAREP.csv'\n",
    "    ]\n",
    "    \n",
    "    df = get_feature_subset_df(required_files=audio_test, folder_ids=get_subset(percentage, random_state))\n",
    "    # Handle duplicate timestamp columns\n",
    "    df['TIMESTAMP'] = df['FORMANT_timestamp']\n",
    "    df = df.drop(columns=[col for col in df.columns if 'timestamp' in col and col != 'TIMESTAMP'])\n",
    "    # Index & timestamp setup\n",
    "    df['TIMESTAMP'] = pd.to_timedelta(df['TIMESTAMP'], unit='s')\n",
    "    df.set_index(['ID', 'TIMESTAMP'], inplace=True)\n",
    "    # down-sampling the high frequency audio data to match the low frequency facial data for multi-modality\n",
    "    df = df.groupby('ID').resample('33.3311ms', level='TIMESTAMP').mean()\n",
    "    # Rounding TIMESTAMP for consistency between audio & face data\n",
    "    df = df.reset_index()\n",
    "    df['TIMESTAMP'] = df['TIMESTAMP'].apply(lambda x: x.round('10ms'))\n",
    "    df.set_index(['ID', 'TIMESTAMP'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#get_audio()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T17:08:36.874597Z",
     "start_time": "2024-11-02T17:08:36.861543Z"
    }
   },
   "id": "ece40569e9f0c0e1",
   "execution_count": 147
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_face(percentage = 0, random_state = 42):\n",
    "    face_test = [\n",
    "        'CLNF_gaze.txt',\n",
    "        'CLNF_AUs.txt',\n",
    "        #'CLNF_hog.bin',\n",
    "        'CLNF_features.txt',\n",
    "        'CLNF_pose.txt',\n",
    "        'CLNF_features3D.txt'\n",
    "    ]\n",
    "    \n",
    "    df = get_feature_subset_df(required_files=face_test, folder_ids=get_subset(percentage, random_state))\n",
    "    # Get rid of unnecessary columns\n",
    "    df = df.drop(columns=[col for col in df.columns if\n",
    "                          any(substring in col for substring in ['frame', 'confidence', 'success'])])\n",
    "    # Handle duplicate timestamp columns\n",
    "    df['TIMESTAMP'] = df['CLNFgaze_timestamp']\n",
    "    df = df.drop(columns=[col for col in df.columns if 'timestamp' in col and col != 'TIMESTAMP'])\n",
    "    # Index & timestamp setup\n",
    "    df['TIMESTAMP'] = pd.to_timedelta(df['TIMESTAMP'], unit='s')\n",
    "    df.set_index(['ID', 'TIMESTAMP'], inplace=True)\n",
    "    # Rounding TIMESTAMP for consistency between audio & face data\n",
    "    df = df.reset_index()\n",
    "    df['TIMESTAMP'] = df['TIMESTAMP'].apply(lambda x: x.round('10ms'))\n",
    "    df.set_index(['ID', 'TIMESTAMP'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#get_face()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T17:08:36.874788Z",
     "start_time": "2024-11-02T17:08:36.865537Z"
    }
   },
   "id": "c224cabc78af2dcd",
   "execution_count": 148
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def sliding_window(df, user_id_col, feature_cols, window_size):\n",
    "    def calculate_rolling_mean(group):\n",
    "        return group[feature_cols].rolling(window=window_size).mean()\n",
    "    \n",
    "    # Apply the sliding window mean to each user group\n",
    "    result_df = df.groupby(user_id_col).apply(calculate_rolling_mean).reset_index(level=0, drop=True)\n",
    "    \n",
    "    # Retain the original user ID and timestamp (if present)\n",
    "    for col in df.columns:\n",
    "        if col != user_id_col and col not in feature_cols:\n",
    "            result_df[col] = df[col]\n",
    "    \n",
    "    return result_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T17:08:36.874963Z",
     "start_time": "2024-11-02T17:08:36.868797Z"
    }
   },
   "id": "f06aabd71318a3f2",
   "execution_count": 149
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

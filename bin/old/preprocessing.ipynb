{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Parsing       "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "221dc828dfaf5fe7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# Define the directory containing the subfolders\n",
    "base_directory = 'data'\n",
    "\n",
    "# Function to retrieve the necessary files\n",
    "def retrieve_files(base_dir, required_files):\n",
    "    # Create a list to store the data for the DataFrame\n",
    "    data = []\n",
    "\n",
    "    # Iterate through each subfolder in the base directory\n",
    "    for subfolder in os.listdir(base_dir):\n",
    "        subfolder_path = os.path.join(base_dir, subfolder)\n",
    "\n",
    "        # Ensure the path is a directory\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            # Create a dictionary to store the file paths for the current subfolder\n",
    "            formatted_subfolder = subfolder.split('_')[0] #getting rid of _P\n",
    "            subfolder_files = {'ID': formatted_subfolder}\n",
    "\n",
    "            # Iterate through each required file\n",
    "            for file_name in required_files:\n",
    "                file_path = os.path.join(subfolder_path, f\"{subfolder[:3]}_{file_name}\")\n",
    "\n",
    "                # Check if the file exists and add it to the dictionary\n",
    "                formatted_file_name = file_name.split('.')[0]\n",
    "                subfolder_files[formatted_file_name] = file_path if os.path.exists(file_path) else None\n",
    "\n",
    "            # Append the dictionary to the data list\n",
    "            data.append(subfolder_files)\n",
    "\n",
    "    # Create a DataFrame from the data list\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T15:56:42.597335Z",
     "start_time": "2024-08-13T15:56:41.812925Z"
    }
   },
   "id": "4f2cb2576218faa9",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#phq_path = 'testing/full_test_split.csv'\n",
    "phq_paths = [\n",
    "    'testing/dev_split_Depression_AVEC2017.csv', \n",
    "    'testing/full_test_split.csv',\n",
    "    'testing/test_split_Depression_AVEC2017.csv',\n",
    "    'testing/train_split_Depression_AVEC2017.csv'\n",
    "]\n",
    "\n",
    "def append_PHQ_Binary(df):\n",
    "    # TODO: make sure to go through all of the testing files\n",
    "    for path in phq_paths:\n",
    "        phq_df = pd.read_csv(path)\n",
    "        phq_df = phq_df[['Participant_ID', 'PHQ_Binary']]\n",
    "    \n",
    "        df = df.merge(phq_df, how='left', left_on='ID', right_on='Participant_ID')\n",
    "        df.drop(columns=['Participant_ID'], inplace=True)\n",
    "        #df.dropna(subset=['PHQ_Binary'], inplace=True)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T15:56:44.404936Z",
     "start_time": "2024-08-13T15:56:44.399290Z"
    }
   },
   "id": "a4c8a73ecdeb913d",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c28269d22b209931"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      ID                     TRANSCRIPT\n0    475  data/475_P/475_TRANSCRIPT.csv\n1    386  data/386_P/386_TRANSCRIPT.csv\n2    361  data/361_P/361_TRANSCRIPT.csv\n3    492  data/492_P/492_TRANSCRIPT.csv\n4    414  data/414_P/414_TRANSCRIPT.csv\n..   ...                            ...\n184  464  data/464_P/464_TRANSCRIPT.csv\n185  420  data/420_P/420_TRANSCRIPT.csv\n186  334  data/334_P/334_TRANSCRIPT.csv\n187  441  data/441_P/441_TRANSCRIPT.csv\n188  355  data/355_P/355_TRANSCRIPT.csv\n\n[189 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>TRANSCRIPT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>475</td>\n      <td>data/475_P/475_TRANSCRIPT.csv</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>386</td>\n      <td>data/386_P/386_TRANSCRIPT.csv</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>361</td>\n      <td>data/361_P/361_TRANSCRIPT.csv</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>492</td>\n      <td>data/492_P/492_TRANSCRIPT.csv</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>414</td>\n      <td>data/414_P/414_TRANSCRIPT.csv</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>464</td>\n      <td>data/464_P/464_TRANSCRIPT.csv</td>\n    </tr>\n    <tr>\n      <th>185</th>\n      <td>420</td>\n      <td>data/420_P/420_TRANSCRIPT.csv</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>334</td>\n      <td>data/334_P/334_TRANSCRIPT.csv</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>441</td>\n      <td>data/441_P/441_TRANSCRIPT.csv</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>355</td>\n      <td>data/355_P/355_TRANSCRIPT.csv</td>\n    </tr>\n  </tbody>\n</table>\n<p>189 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_files = [\n",
    "    'TRANSCRIPT.csv'\n",
    "]\n",
    "\n",
    "df_text_files = retrieve_files(base_directory, text_files)\n",
    "df_text_files"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T18:25:18.529052Z",
     "start_time": "2024-07-30T18:25:18.513183Z"
    }
   },
   "id": "83df7daae1445b96",
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_TRANSCRIPT(path):\n",
    "    df = pd.read_csv(path, delimiter='\\t')\n",
    "    # Combine all rows of the 'value' column into one string\n",
    "    transcript = ' '.join(df['value'].astype(str).tolist())\n",
    "    # Clean the text\n",
    "    transcript = re.sub(r'[^\\w\\s]', '', transcript.lower())\n",
    "    return transcript\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T18:25:18.540709Z",
     "start_time": "2024-07-30T18:25:18.532397Z"
    }
   },
   "id": "48103df971774e79",
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_text = []\n",
    "\n",
    "for i in df_text_files:\n",
    "    # preprocess all the features\n",
    "    transcript = preprocess_TRANSCRIPT(i.TRANSCRIPT)\n",
    "\n",
    "    # concatenate all the features and add ID \n",
    "    df_concat['ID'] = df_text_files['ID']\n",
    "\n",
    "    # add PHQ binary\n",
    "    df_concat = append_PHQ_Binary(df_concat)\n",
    "    \n",
    "    # append\n",
    "    df_text.append(df_concat)\n",
    "    \n",
    "df_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T18:25:18.550584Z",
     "start_time": "2024-07-30T18:25:18.534943Z"
    }
   },
   "id": "9a235cb00e5cf18e",
   "execution_count": 89
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing audio"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ac01dfa92a8bbcd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      ID                     AUDIO                     FORMANT  \\\n0    475  data/475_P/475_AUDIO.wav  data/475_P/475_FORMANT.csv   \n1    386  data/386_P/386_AUDIO.wav  data/386_P/386_FORMANT.csv   \n2    361  data/361_P/361_AUDIO.wav  data/361_P/361_FORMANT.csv   \n3    492  data/492_P/492_AUDIO.wav  data/492_P/492_FORMANT.csv   \n4    414  data/414_P/414_AUDIO.wav  data/414_P/414_FORMANT.csv   \n..   ...                       ...                         ...   \n184  464  data/464_P/464_AUDIO.wav  data/464_P/464_FORMANT.csv   \n185  420  data/420_P/420_AUDIO.wav  data/420_P/420_FORMANT.csv   \n186  334  data/334_P/334_AUDIO.wav  data/334_P/334_FORMANT.csv   \n187  441  data/441_P/441_AUDIO.wav  data/441_P/441_FORMANT.csv   \n188  355  data/355_P/355_AUDIO.wav  data/355_P/355_FORMANT.csv   \n\n                        COVAREP  \n0    data/475_P/475_COVAREP.csv  \n1    data/386_P/386_COVAREP.csv  \n2    data/361_P/361_COVAREP.csv  \n3    data/492_P/492_COVAREP.csv  \n4    data/414_P/414_COVAREP.csv  \n..                          ...  \n184  data/464_P/464_COVAREP.csv  \n185  data/420_P/420_COVAREP.csv  \n186  data/334_P/334_COVAREP.csv  \n187  data/441_P/441_COVAREP.csv  \n188  data/355_P/355_COVAREP.csv  \n\n[189 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>AUDIO</th>\n      <th>FORMANT</th>\n      <th>COVAREP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>475</td>\n      <td>data/475_P/475_AUDIO.wav</td>\n      <td>data/475_P/475_FORMANT.csv</td>\n      <td>data/475_P/475_COVAREP.csv</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>386</td>\n      <td>data/386_P/386_AUDIO.wav</td>\n      <td>data/386_P/386_FORMANT.csv</td>\n      <td>data/386_P/386_COVAREP.csv</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>361</td>\n      <td>data/361_P/361_AUDIO.wav</td>\n      <td>data/361_P/361_FORMANT.csv</td>\n      <td>data/361_P/361_COVAREP.csv</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>492</td>\n      <td>data/492_P/492_AUDIO.wav</td>\n      <td>data/492_P/492_FORMANT.csv</td>\n      <td>data/492_P/492_COVAREP.csv</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>414</td>\n      <td>data/414_P/414_AUDIO.wav</td>\n      <td>data/414_P/414_FORMANT.csv</td>\n      <td>data/414_P/414_COVAREP.csv</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>464</td>\n      <td>data/464_P/464_AUDIO.wav</td>\n      <td>data/464_P/464_FORMANT.csv</td>\n      <td>data/464_P/464_COVAREP.csv</td>\n    </tr>\n    <tr>\n      <th>185</th>\n      <td>420</td>\n      <td>data/420_P/420_AUDIO.wav</td>\n      <td>data/420_P/420_FORMANT.csv</td>\n      <td>data/420_P/420_COVAREP.csv</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>334</td>\n      <td>data/334_P/334_AUDIO.wav</td>\n      <td>data/334_P/334_FORMANT.csv</td>\n      <td>data/334_P/334_COVAREP.csv</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>441</td>\n      <td>data/441_P/441_AUDIO.wav</td>\n      <td>data/441_P/441_FORMANT.csv</td>\n      <td>data/441_P/441_COVAREP.csv</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>355</td>\n      <td>data/355_P/355_AUDIO.wav</td>\n      <td>data/355_P/355_FORMANT.csv</td>\n      <td>data/355_P/355_COVAREP.csv</td>\n    </tr>\n  </tbody>\n</table>\n<p>189 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_files = [\n",
    "    'AUDIO.wav',\n",
    "    'FORMANT.csv',\n",
    "    'COVAREP.csv'\n",
    "]\n",
    "\n",
    "df_audio_files = retrieve_files(base_directory, audio_files)\n",
    "df_audio_files"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T18:25:18.561801Z",
     "start_time": "2024-07-30T18:25:18.537899Z"
    }
   },
   "id": "32db37586e1900bf",
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "\n",
    "def preprocess_AUDIO(path):\n",
    "    y, sr = librosa.load(path, sr=None)\n",
    "\n",
    "    # Extract various features\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "\n",
    "    # Aggregate the features\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)\n",
    "    chroma_mean = np.mean(chroma, axis=1)\n",
    "    zcr_mean = np.mean(zcr, axis=1)\n",
    "\n",
    "    a = np.concatenate((mfccs_mean, chroma_mean, zcr_mean), axis=0)\n",
    "    return 0\n",
    "\n",
    "def preprocess_FORMANT(path):\n",
    "    # formant_features = pd.read_csv(path, delimiter='\\t')\n",
    "    # return formant_features\n",
    "\n",
    "\n",
    "def preprocess_COVAREP(path):\n",
    "    # TODO: I probably need to use the COVAREP library for this\n",
    "    # covarep_features = pd.read_csv(path, delimiter='\\t')\n",
    "    # return covarep_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T18:25:18.562151Z",
     "start_time": "2024-07-30T18:25:18.547726Z"
    }
   },
   "id": "d9a5a836b9bfa50e",
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_audio = []\n",
    "\n",
    "for i in df_audio_files:\n",
    "    # preprocess all the features\n",
    "    audio = preprocess_AUDIO(i.AUDIO)\n",
    "    formant = preprocess_FORMANT(i.FORMANT)\n",
    "    covarep = preprocess_COVAREP(i.COVAREP)\n",
    "\n",
    "    # concatenate all the features and add ID \n",
    "    df_concat = pd.concat([audio, formant, covarep], axis=1)\n",
    "    df_concat['ID'] = df_audio_files['ID']\n",
    "    \n",
    "    # add PHQ binary\n",
    "    df_concat = append_PHQ_Binary(df_concat)\n",
    "    \n",
    "    # append\n",
    "    df_audio.append(df_concat)\n",
    "\n",
    "df_audio"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T18:25:18.562336Z",
     "start_time": "2024-07-30T18:25:18.549418Z"
    }
   },
   "id": "c8298b94d4fbfb72",
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "           F1      F2      F3      F4      F5\n0      753.36  2405.3  3081.5  3865.9  4280.8\n1      812.41  2203.2  3121.1  3598.4  4116.2\n2      827.90  2176.6  3047.9  3523.0  4095.6\n3      832.05  2090.4  2807.2  3303.3  4184.8\n4      882.31  2097.5  2636.9  3078.0  4321.1\n...       ...     ...     ...     ...     ...\n64845  457.00  1473.0  2597.5  3160.0  3629.0\n64846  500.00  1527.5  2562.5  3261.5  4059.0\n64847  523.50  1566.5  2461.0  3429.5  4566.5\n64848  531.00  1531.5  2406.0  3554.5  4679.5\n64849  531.00  1469.0  2410.0  3773.5  4734.5\n\n[64850 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F1</th>\n      <th>F2</th>\n      <th>F3</th>\n      <th>F4</th>\n      <th>F5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>753.36</td>\n      <td>2405.3</td>\n      <td>3081.5</td>\n      <td>3865.9</td>\n      <td>4280.8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>812.41</td>\n      <td>2203.2</td>\n      <td>3121.1</td>\n      <td>3598.4</td>\n      <td>4116.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>827.90</td>\n      <td>2176.6</td>\n      <td>3047.9</td>\n      <td>3523.0</td>\n      <td>4095.6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>832.05</td>\n      <td>2090.4</td>\n      <td>2807.2</td>\n      <td>3303.3</td>\n      <td>4184.8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>882.31</td>\n      <td>2097.5</td>\n      <td>2636.9</td>\n      <td>3078.0</td>\n      <td>4321.1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>64845</th>\n      <td>457.00</td>\n      <td>1473.0</td>\n      <td>2597.5</td>\n      <td>3160.0</td>\n      <td>3629.0</td>\n    </tr>\n    <tr>\n      <th>64846</th>\n      <td>500.00</td>\n      <td>1527.5</td>\n      <td>2562.5</td>\n      <td>3261.5</td>\n      <td>4059.0</td>\n    </tr>\n    <tr>\n      <th>64847</th>\n      <td>523.50</td>\n      <td>1566.5</td>\n      <td>2461.0</td>\n      <td>3429.5</td>\n      <td>4566.5</td>\n    </tr>\n    <tr>\n      <th>64848</th>\n      <td>531.00</td>\n      <td>1531.5</td>\n      <td>2406.0</td>\n      <td>3554.5</td>\n      <td>4679.5</td>\n    </tr>\n    <tr>\n      <th>64849</th>\n      <td>531.00</td>\n      <td>1469.0</td>\n      <td>2410.0</td>\n      <td>3773.5</td>\n      <td>4734.5</td>\n    </tr>\n  </tbody>\n</table>\n<p>64850 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "        \n",
    "df = pd.read_csv('data/300_P/300_FORMANT.csv', header=None)\n",
    "FORMANT_FEATURES = ['F1', 'F2', 'F3', 'F4', 'F5']\n",
    "df.columns = FORMANT_FEATURES\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T14:37:37.328945Z",
     "start_time": "2024-08-15T14:37:37.308125Z"
    }
   },
   "id": "c859e1bd14c06493",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "        F0  VUV  NAQ  QOQ  H1H2  PSP  MDQ  peakSlope   Rd  Rd_conf  ...  \\\n0      0.0    0  0.0  0.0   0.0  0.0  0.0    0.00000  0.0      0.0  ...   \n1      0.0    0  0.0  0.0   0.0  0.0  0.0    0.00000  0.0      0.0  ...   \n2      0.0    0  0.0  0.0   0.0  0.0  0.0   -0.30805  0.0      0.0  ...   \n3      0.0    0  0.0  0.0   0.0  0.0  0.0   -0.30553  0.0      0.0  ...   \n4      0.0    0  0.0  0.0   0.0  0.0  0.0   -0.32064  0.0      0.0  ...   \n...    ...  ...  ...  ...   ...  ...  ...        ...  ...      ...  ...   \n64846  0.0    0  0.0  0.0   0.0  0.0  0.0   -0.44373  0.0      0.0  ...   \n64847  0.0    0  0.0  0.0   0.0  0.0  0.0   -0.44239  0.0      0.0  ...   \n64848  0.0    0  0.0  0.0   0.0  0.0  0.0    0.00000  0.0      0.0  ...   \n64849  0.0    0  0.0  0.0   0.0  0.0  0.0    0.00000  0.0      0.0  ...   \n64850  0.0    0  0.0  0.0   0.0  0.0  0.0    0.00000  0.0      0.0  ...   \n\n       HMPDD_4  HMPDD_5  HMPDD_6  HMPDD_7  HMPDD_8  HMPDD_9  HMPDD_10  \\\n0          0.0      0.0      0.0      0.0      0.0      0.0       0.0   \n1          0.0      0.0      0.0      0.0      0.0      0.0       0.0   \n2          0.0      0.0      0.0      0.0      0.0      0.0       0.0   \n3          0.0      0.0      0.0      0.0      0.0      0.0       0.0   \n4          0.0      0.0      0.0      0.0      0.0      0.0       0.0   \n...        ...      ...      ...      ...      ...      ...       ...   \n64846      0.0      0.0      0.0      0.0      0.0      0.0       0.0   \n64847      0.0      0.0      0.0      0.0      0.0      0.0       0.0   \n64848      0.0      0.0      0.0      0.0      0.0      0.0       0.0   \n64849      0.0      0.0      0.0      0.0      0.0      0.0       0.0   \n64850      0.0      0.0      0.0      0.0      0.0      0.0       0.0   \n\n       HMPDD_11  HMPDD_12  HMPDD_13  \n0           0.0       0.0       0.0  \n1           0.0       0.0       0.0  \n2           0.0       0.0       0.0  \n3           0.0       0.0       0.0  \n4           0.0       0.0       0.0  \n...         ...       ...       ...  \n64846       0.0       0.0       0.0  \n64847       0.0       0.0       0.0  \n64848       0.0       0.0       0.0  \n64849       0.0       0.0       0.0  \n64850       0.0       0.0       0.0  \n\n[64851 rows x 74 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F0</th>\n      <th>VUV</th>\n      <th>NAQ</th>\n      <th>QOQ</th>\n      <th>H1H2</th>\n      <th>PSP</th>\n      <th>MDQ</th>\n      <th>peakSlope</th>\n      <th>Rd</th>\n      <th>Rd_conf</th>\n      <th>...</th>\n      <th>HMPDD_4</th>\n      <th>HMPDD_5</th>\n      <th>HMPDD_6</th>\n      <th>HMPDD_7</th>\n      <th>HMPDD_8</th>\n      <th>HMPDD_9</th>\n      <th>HMPDD_10</th>\n      <th>HMPDD_11</th>\n      <th>HMPDD_12</th>\n      <th>HMPDD_13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.30805</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.30553</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.32064</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>64846</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.44373</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>64847</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.44239</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>64848</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>64849</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>64850</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>64851 rows × 74 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "        \n",
    "df = pd.read_csv('data/300_P/300_COVAREP.csv', header=None)\n",
    "# something is off here\n",
    "COVAREP_FEATURES = [\n",
    "    'F0',\n",
    "    'VUV',\n",
    "    'NAQ',\n",
    "    'QOQ',\n",
    "    'H1H2',\n",
    "    'PSP',\n",
    "    'MDQ',\n",
    "    'peakSlope',\n",
    "    'Rd',\n",
    "    'Rd_conf',\n",
    "    \n",
    "    'MCEP_0',\n",
    "    'MCEP_1',\n",
    "    'MCEP_2',\n",
    "    'MCEP_3',\n",
    "    'MCEP_4',\n",
    "    'MCEP_5',\n",
    "    'MCEP_6',\n",
    "    'MCEP_7',\n",
    "    'MCEP_8',\n",
    "    'MCEP_9',\n",
    "    'MCEP_10',\n",
    "    'MCEP_11',\n",
    "    'MCEP_12',\n",
    "    'MCEP_13',\n",
    "    'MCEP_14',\n",
    "    'MCEP_15',\n",
    "    'MCEP_16',\n",
    "    'MCEP_17',\n",
    "    'MCEP_18',\n",
    "    'MCEP_19',\n",
    "    'MCEP_20',\n",
    "    'MCEP_21',\n",
    "    'MCEP_22',\n",
    "    'MCEP_23',\n",
    "    'MCEP_24',\n",
    "\n",
    "    'HMPDM_0',\n",
    "    'HMPDM_1',\n",
    "    'HMPDM_2',\n",
    "    'HMPDM_3',\n",
    "    'HMPDM_4',\n",
    "    'HMPDM_5',\n",
    "    'HMPDM_6',\n",
    "    'HMPDM_7',\n",
    "    'HMPDM_8',\n",
    "    'HMPDM_9',\n",
    "    'HMPDM_10',\n",
    "    'HMPDM_11',\n",
    "    'HMPDM_12',\n",
    "    'HMPDM_13',\n",
    "    'HMPDM_14',\n",
    "    'HMPDM_15',\n",
    "    'HMPDM_16',\n",
    "    'HMPDM_17',\n",
    "    'HMPDM_18',\n",
    "    'HMPDM_19',\n",
    "    'HMPDM_20',\n",
    "    'HMPDM_21',\n",
    "    'HMPDM_22',\n",
    "    'HMPDM_23',\n",
    "    'HMPDM_24',\n",
    "\n",
    "    'HMPDD_0',\n",
    "    'HMPDD_1',\n",
    "    'HMPDD_2',\n",
    "    'HMPDD_3',\n",
    "    'HMPDD_4',\n",
    "    'HMPDD_5',\n",
    "    'HMPDD_6',\n",
    "    'HMPDD_7',\n",
    "    'HMPDD_8',\n",
    "    'HMPDD_9',\n",
    "    'HMPDD_10',\n",
    "    'HMPDD_11',\n",
    "    'HMPDD_12',\n",
    "    'HMPDD_13',\n",
    "]\n",
    "df.columns = COVAREP_FEATURES\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T14:37:40.732033Z",
     "start_time": "2024-08-15T14:37:40.491117Z"
    }
   },
   "id": "b8383d349fcef504",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing face"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e15e567b3405cd6f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      ID                     CLNF_gaze                     CLNF_AUs  \\\n0    475  data/475_P/475_CLNF_gaze.txt  data/475_P/475_CLNF_AUs.txt   \n1    386  data/386_P/386_CLNF_gaze.txt  data/386_P/386_CLNF_AUs.txt   \n2    361  data/361_P/361_CLNF_gaze.txt  data/361_P/361_CLNF_AUs.txt   \n3    492  data/492_P/492_CLNF_gaze.txt  data/492_P/492_CLNF_AUs.txt   \n4    414  data/414_P/414_CLNF_gaze.txt  data/414_P/414_CLNF_AUs.txt   \n..   ...                           ...                          ...   \n184  464  data/464_P/464_CLNF_gaze.txt  data/464_P/464_CLNF_AUs.txt   \n185  420  data/420_P/420_CLNF_gaze.txt  data/420_P/420_CLNF_AUs.txt   \n186  334  data/334_P/334_CLNF_gaze.txt  data/334_P/334_CLNF_AUs.txt   \n187  441  data/441_P/441_CLNF_gaze.txt  data/441_P/441_CLNF_AUs.txt   \n188  355  data/355_P/355_CLNF_gaze.txt  data/355_P/355_CLNF_AUs.txt   \n\n                        CLNF_hog                     CLNF_features  \\\n0    data/475_P/475_CLNF_hog.bin  data/475_P/475_CLNF_features.txt   \n1    data/386_P/386_CLNF_hog.bin  data/386_P/386_CLNF_features.txt   \n2    data/361_P/361_CLNF_hog.bin  data/361_P/361_CLNF_features.txt   \n3    data/492_P/492_CLNF_hog.bin  data/492_P/492_CLNF_features.txt   \n4    data/414_P/414_CLNF_hog.bin  data/414_P/414_CLNF_features.txt   \n..                           ...                               ...   \n184  data/464_P/464_CLNF_hog.bin  data/464_P/464_CLNF_features.txt   \n185  data/420_P/420_CLNF_hog.bin  data/420_P/420_CLNF_features.txt   \n186  data/334_P/334_CLNF_hog.bin  data/334_P/334_CLNF_features.txt   \n187  data/441_P/441_CLNF_hog.bin  data/441_P/441_CLNF_features.txt   \n188  data/355_P/355_CLNF_hog.bin  data/355_P/355_CLNF_features.txt   \n\n                        CLNF_pose                     CLNF_features3D  \n0    data/475_P/475_CLNF_pose.txt  data/475_P/475_CLNF_features3D.txt  \n1    data/386_P/386_CLNF_pose.txt  data/386_P/386_CLNF_features3D.txt  \n2    data/361_P/361_CLNF_pose.txt  data/361_P/361_CLNF_features3D.txt  \n3    data/492_P/492_CLNF_pose.txt  data/492_P/492_CLNF_features3D.txt  \n4    data/414_P/414_CLNF_pose.txt  data/414_P/414_CLNF_features3D.txt  \n..                            ...                                 ...  \n184  data/464_P/464_CLNF_pose.txt  data/464_P/464_CLNF_features3D.txt  \n185  data/420_P/420_CLNF_pose.txt  data/420_P/420_CLNF_features3D.txt  \n186  data/334_P/334_CLNF_pose.txt  data/334_P/334_CLNF_features3D.txt  \n187  data/441_P/441_CLNF_pose.txt  data/441_P/441_CLNF_features3D.txt  \n188  data/355_P/355_CLNF_pose.txt  data/355_P/355_CLNF_features3D.txt  \n\n[189 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>CLNF_gaze</th>\n      <th>CLNF_AUs</th>\n      <th>CLNF_hog</th>\n      <th>CLNF_features</th>\n      <th>CLNF_pose</th>\n      <th>CLNF_features3D</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>475</td>\n      <td>data/475_P/475_CLNF_gaze.txt</td>\n      <td>data/475_P/475_CLNF_AUs.txt</td>\n      <td>data/475_P/475_CLNF_hog.bin</td>\n      <td>data/475_P/475_CLNF_features.txt</td>\n      <td>data/475_P/475_CLNF_pose.txt</td>\n      <td>data/475_P/475_CLNF_features3D.txt</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>386</td>\n      <td>data/386_P/386_CLNF_gaze.txt</td>\n      <td>data/386_P/386_CLNF_AUs.txt</td>\n      <td>data/386_P/386_CLNF_hog.bin</td>\n      <td>data/386_P/386_CLNF_features.txt</td>\n      <td>data/386_P/386_CLNF_pose.txt</td>\n      <td>data/386_P/386_CLNF_features3D.txt</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>361</td>\n      <td>data/361_P/361_CLNF_gaze.txt</td>\n      <td>data/361_P/361_CLNF_AUs.txt</td>\n      <td>data/361_P/361_CLNF_hog.bin</td>\n      <td>data/361_P/361_CLNF_features.txt</td>\n      <td>data/361_P/361_CLNF_pose.txt</td>\n      <td>data/361_P/361_CLNF_features3D.txt</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>492</td>\n      <td>data/492_P/492_CLNF_gaze.txt</td>\n      <td>data/492_P/492_CLNF_AUs.txt</td>\n      <td>data/492_P/492_CLNF_hog.bin</td>\n      <td>data/492_P/492_CLNF_features.txt</td>\n      <td>data/492_P/492_CLNF_pose.txt</td>\n      <td>data/492_P/492_CLNF_features3D.txt</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>414</td>\n      <td>data/414_P/414_CLNF_gaze.txt</td>\n      <td>data/414_P/414_CLNF_AUs.txt</td>\n      <td>data/414_P/414_CLNF_hog.bin</td>\n      <td>data/414_P/414_CLNF_features.txt</td>\n      <td>data/414_P/414_CLNF_pose.txt</td>\n      <td>data/414_P/414_CLNF_features3D.txt</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>464</td>\n      <td>data/464_P/464_CLNF_gaze.txt</td>\n      <td>data/464_P/464_CLNF_AUs.txt</td>\n      <td>data/464_P/464_CLNF_hog.bin</td>\n      <td>data/464_P/464_CLNF_features.txt</td>\n      <td>data/464_P/464_CLNF_pose.txt</td>\n      <td>data/464_P/464_CLNF_features3D.txt</td>\n    </tr>\n    <tr>\n      <th>185</th>\n      <td>420</td>\n      <td>data/420_P/420_CLNF_gaze.txt</td>\n      <td>data/420_P/420_CLNF_AUs.txt</td>\n      <td>data/420_P/420_CLNF_hog.bin</td>\n      <td>data/420_P/420_CLNF_features.txt</td>\n      <td>data/420_P/420_CLNF_pose.txt</td>\n      <td>data/420_P/420_CLNF_features3D.txt</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>334</td>\n      <td>data/334_P/334_CLNF_gaze.txt</td>\n      <td>data/334_P/334_CLNF_AUs.txt</td>\n      <td>data/334_P/334_CLNF_hog.bin</td>\n      <td>data/334_P/334_CLNF_features.txt</td>\n      <td>data/334_P/334_CLNF_pose.txt</td>\n      <td>data/334_P/334_CLNF_features3D.txt</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>441</td>\n      <td>data/441_P/441_CLNF_gaze.txt</td>\n      <td>data/441_P/441_CLNF_AUs.txt</td>\n      <td>data/441_P/441_CLNF_hog.bin</td>\n      <td>data/441_P/441_CLNF_features.txt</td>\n      <td>data/441_P/441_CLNF_pose.txt</td>\n      <td>data/441_P/441_CLNF_features3D.txt</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>355</td>\n      <td>data/355_P/355_CLNF_gaze.txt</td>\n      <td>data/355_P/355_CLNF_AUs.txt</td>\n      <td>data/355_P/355_CLNF_hog.bin</td>\n      <td>data/355_P/355_CLNF_features.txt</td>\n      <td>data/355_P/355_CLNF_pose.txt</td>\n      <td>data/355_P/355_CLNF_features3D.txt</td>\n    </tr>\n  </tbody>\n</table>\n<p>189 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_files = [\n",
    "    'CLNF_gaze.txt',\n",
    "    'CLNF_AUs.txt',\n",
    "    'CLNF_hog.bin', #TODO: 300 is a .txt file, not a .bin file\n",
    "    'CLNF_features.txt',\n",
    "    'CLNF_pose.txt',\n",
    "    'CLNF_features3D.txt',\n",
    "]\n",
    "\n",
    "df_face_files = retrieve_files(base_directory, face_files)\n",
    "df_face_files"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T18:25:18.590097Z",
     "start_time": "2024-07-30T18:25:18.552180Z"
    }
   },
   "id": "fe3deb4b27e50635",
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def preprocess_CLNF_gaze(path):\n",
    "    gaze_df = pd.read_csv(path)\n",
    "    return gaze_df\n",
    "\n",
    "def preprocess_CLNF_AUs(path):\n",
    "    au_df = pd.read_csv(path)\n",
    "    return au_df\n",
    "\n",
    "def preprocess_CLNF_hog(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        curr_data = []\n",
    "        curr_ind = 0\n",
    "\n",
    "        while True:\n",
    "            if curr_ind == 0:\n",
    "                num_cols = np.fromfile(f, dtype=np.int32, count=1)\n",
    "                if num_cols.size == 0:\n",
    "                    break\n",
    "\n",
    "                num_rows = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "                num_chan = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "\n",
    "                curr_ind += 1\n",
    "\n",
    "                if curr_ind == 1:\n",
    "                    curr_data = np.zeros((1000, 1 + num_rows * num_cols * num_chan))\n",
    "                    num_feats = 1 + num_rows * num_cols * num_chan\n",
    "\n",
    "                if curr_ind > curr_data.shape[0]:\n",
    "                    curr_data = np.vstack([curr_data, np.zeros((1000, num_feats))])\n",
    "\n",
    "                feature_vec = np.fromfile(f, dtype=np.float32, count=1 + num_rows * num_cols * num_chan)\n",
    "                curr_data[curr_ind - 1, :] = feature_vec\n",
    "            else:\n",
    "                feature_vec = np.fromfile(f, dtype=np.float32, count=(4 + num_rows * num_cols * num_chan) * 5000)\n",
    "                feature_vec = feature_vec.reshape((-1, 4 + num_rows * num_cols * num_chan))[:, 4:]\n",
    "\n",
    "                num_rows_read = feature_vec.shape[0]\n",
    "\n",
    "                if feature_vec.size > 0:\n",
    "                    if curr_ind + num_rows_read > curr_data.shape[0]:\n",
    "                        curr_data = np.vstack([curr_data, np.zeros((num_rows_read, num_feats))])\n",
    "                    curr_data[curr_ind:curr_ind + num_rows_read, :] = feature_vec\n",
    "                    curr_ind += num_rows_read\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        curr_data = curr_data[:curr_ind, :]\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(curr_data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_CLNF_features(path):\n",
    "    features_df = pd.read_csv(path)\n",
    "    return features_df\n",
    "\n",
    "def preprocess_CLNF_pose(path):\n",
    "    pose_df = pd.read_csv(path)\n",
    "    return pose_df\n",
    "\n",
    "def preprocess_CLNF_features3D(path):\n",
    "    features3D_df = pd.read_csv(path)\n",
    "    return features3D_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T14:18:08.442537Z",
     "start_time": "2024-08-16T14:18:08.435425Z"
    }
   },
   "id": "f7d0cc0d0c1a2100",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_face = []\n",
    "\n",
    "for i in df_face_files:\n",
    "    # preprocess all the features\n",
    "    clnf_gaze = preprocess_CLNF_gaze(i.CLNF_gaze)\n",
    "    clnf_aus = preprocess_CLNF_AUs(i.CLNF_AUs)\n",
    "    clnf_hog = preprocess_CLNF_hog(i.CLNF_hog)\n",
    "    clnf_features = preprocess_CLNF_features(i.CLNF_features)\n",
    "    clnf_pose = preprocess_CLNF_pose(i.CLNF_pose)\n",
    "    clnf_features3d = preprocess_CLNF_features3D(i.CLNF_features3D)\n",
    "    \n",
    "    # concatenate all the features and add ID \n",
    "    df_concat = pd.concat([clnf_gaze, clnf_aus, clnf_hog, clnf_features, clnf_pose, clnf_features3d], axis=1)\n",
    "    df_concat['ID'] = df_face_files['ID']\n",
    "    \n",
    "    # add PHQ binary\n",
    "    df_concat = append_PHQ_Binary(df_concat)\n",
    "    \n",
    "    # append\n",
    "    df_face.append(df_concat)\n",
    "    \n",
    "df_face\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T18:25:18.590653Z",
     "start_time": "2024-07-30T18:25:18.567282Z"
    }
   },
   "id": "6728abadce978c0a",
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       is_valid                                       hog_features\n",
      "0           1.0  [0.2149130403995514, 0.29190343618392944, 0.40...\n",
      "1           1.0  [0.1428118348121643, 0.2792850732803345, 0.400...\n",
      "2           1.0  [0.16635194420814514, 0.2612675428390503, 0.40...\n",
      "3           1.0  [0.10758522897958755, 0.2900768220424652, 0.40...\n",
      "4           1.0  [0.08714132010936737, 0.2694597542285919, 0.40...\n",
      "...         ...                                                ...\n",
      "24716      -1.0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "24717      -1.0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "24718      -1.0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "24719      -1.0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "24720      -1.0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "\n",
      "[24721 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# READ HOG FILES\n",
    "\n",
    "import numpy as np\n",
    "import struct\n",
    "import pandas as pd\n",
    "\n",
    "def read_hog(filename, batch_size=5000):\n",
    "    all_feature_vectors = []\n",
    "    with open(filename, \"rb\") as f:\n",
    "        num_cols, = struct.unpack(\"i\", f.read(4))\n",
    "        num_rows, = struct.unpack(\"i\", f.read(4))\n",
    "        num_channels, = struct.unpack(\"i\", f.read(4))\n",
    "\n",
    "        # The first four bytes encode a boolean value whether the frame is valid\n",
    "        num_features = 1 + num_rows * num_cols * num_channels\n",
    "        feature_vector = struct.unpack(\"{}f\".format(num_features), f.read(num_features * 4))\n",
    "        feature_vector = np.array(feature_vector).reshape((1, num_features))\n",
    "        all_feature_vectors.append(feature_vector)\n",
    "\n",
    "        # Every frame contains a header of four float values: num_cols, num_rows, num_channels, is_valid\n",
    "        num_floats_per_feature_vector = 4 + num_rows * num_cols * num_channels\n",
    "        # Read in batches of given batch_size\n",
    "        num_floats_to_read = num_floats_per_feature_vector * batch_size\n",
    "        # Multiply by 4 because of float32\n",
    "        num_bytes_to_read = num_floats_to_read * 4\n",
    "\n",
    "        while True:\n",
    "            bytes = f.read(num_bytes_to_read)\n",
    "            # For comparison how many bytes were actually read\n",
    "            num_bytes_read = len(bytes)\n",
    "            if num_bytes_read == 0:\n",
    "                break\n",
    "\n",
    "            assert num_bytes_read % 4 == 0, \"Number of bytes read does not match with float size\"\n",
    "            num_floats_read = num_bytes_read // 4\n",
    "            assert num_floats_read % num_floats_per_feature_vector == 0, \"Number of bytes read does not match with feature vector size\"\n",
    "            num_feature_vectors_read = num_floats_read // num_floats_per_feature_vector\n",
    "\n",
    "            feature_vectors = struct.unpack(\"{}f\".format(num_floats_read), bytes)\n",
    "            # Convert to array\n",
    "            feature_vectors = np.array(feature_vectors).reshape((num_feature_vectors_read, num_floats_per_feature_vector))\n",
    "            # Discard the first three values in each row (num_cols, num_rows, num_channels)\n",
    "            feature_vectors = feature_vectors[:, 3:]\n",
    "            # Append to list of all feature vectors that have been read so far\n",
    "            all_feature_vectors.append(feature_vectors)\n",
    "\n",
    "            if num_bytes_read < num_bytes_to_read:\n",
    "                break\n",
    "\n",
    "        # Concatenate batches\n",
    "        all_feature_vectors = np.concatenate(all_feature_vectors, axis=0)\n",
    "\n",
    "        # Split into is-valid and feature vectors\n",
    "        is_valid = all_feature_vectors[:, 0]\n",
    "        hog_features = all_feature_vectors[:, 1:]\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'is_valid': is_valid,\n",
    "            'hog_features': list(hog_features)  # Store each row as a list in a DataFrame cell\n",
    "        })\n",
    "\n",
    "        return df\n",
    "    \n",
    "# Example usage\n",
    "df = read_hog('data/301_P/301_CLNF_hog.bin')\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T15:27:45.346846Z",
     "start_time": "2024-09-09T15:27:41.003018Z"
    }
   },
   "id": "541f0c8c32455b1e",
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

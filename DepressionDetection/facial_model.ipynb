{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29f71f6",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73765f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the data\n",
    "data_path = 'data_face'\n",
    "\n",
    "# List all files in the data directory\n",
    "all_files = glob.glob(os.path.join(data_path, '*'))\n",
    "\n",
    "# Function to load data from text files\n",
    "def load_text_file(file_path):\n",
    "    return pd.read_csv(file_path, delim_whitespace=True)\n",
    "\n",
    "# Function to load data from binary HOG files\n",
    "def load_hog_file(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = np.fromfile(f, dtype=np.float32)\n",
    "    return data\n",
    "\n",
    "# Dictionary to store data for each person\n",
    "data = {}\n",
    "\n",
    "# Load each type of file\n",
    "for file_path in all_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    person_id = file_name.split('_')[0]\n",
    "    \n",
    "    if person_id not in data:\n",
    "        data[person_id] = {}\n",
    "    \n",
    "    if file_name.endswith('.txt'):\n",
    "        data[person_id][file_name] = load_text_file(file_path)\n",
    "    elif file_name.endswith('.bin'):\n",
    "        data[person_id][file_name] = load_hog_file(file_path)\n",
    "\n",
    "# Function to preprocess text data\n",
    "def preprocess_text_data(df):\n",
    "    # Impute missing values with mean\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df_imputed = imputer.fit_transform(df)\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_imputed)\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "# Preprocess the data for each person\n",
    "for person_id, person_data in data.items():\n",
    "    for file_name, df in person_data.items():\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            data[person_id][file_name] = preprocess_text_data(df)\n",
    "\n",
    "# Example of accessing preprocessed data for a specific person\n",
    "person_id = '301'\n",
    "print(data[person_id].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f707c132",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualize the distribution of a specific feature\n",
    "def plot_feature_distribution(data, feature_name):\n",
    "    feature_values = []\n",
    "    for person_id, person_data in data.items():\n",
    "        for file_name, df in person_data.items():\n",
    "            if isinstance(df, np.ndarray):  # Only for numpy arrays (text data)\n",
    "                feature_values.append(df[:, feature_name])\n",
    "    \n",
    "    feature_values = np.concatenate(feature_values)\n",
    "    sns.histplot(feature_values, kde=True)\n",
    "    plt.title(f'Distribution of {feature_name}')\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# Plotting a feature (adjust feature_name as needed)\n",
    "plot_feature_distribution(data, 0)  # Example for the first feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bde188",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dfa2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features\n",
    "def extract_features(person_data):\n",
    "    features = []\n",
    "    \n",
    "    for file_name, df in person_data.items():\n",
    "        if isinstance(df, np.ndarray):  # HOG data\n",
    "            features.append(df)\n",
    "        elif isinstance(df, np.ndarray):  # Other preprocessed text data\n",
    "            features.append(df.flatten())\n",
    "    \n",
    "    return np.concatenate(features)\n",
    "\n",
    "# Create feature vectors for each person\n",
    "feature_vectors = []\n",
    "labels = []  # Assuming you have labels indicating depression or not\n",
    "\n",
    "for person_id, person_data in data.items():\n",
    "    features = extract_features(person_data)\n",
    "    feature_vectors.append(features)\n",
    "    # Append the label for this person\n",
    "    labels.append(0)  # Replace with actual label\n",
    "\n",
    "X = np.array(feature_vectors)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac006a0e",
   "metadata": {},
   "source": [
    "# Creating the pipeline & hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b16f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear']\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Best Score: {grid_search.best_score_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f20b393",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

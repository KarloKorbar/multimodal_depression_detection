{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class MultimodalFusionDataset(Dataset):\n",
    "    def __init__(self, text_features, audio_features, face_features, labels):\n",
    "        self.text_features = text_features\n",
    "        self.audio_features = audio_features \n",
    "        self.face_features = face_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text': torch.FloatTensor(self.text_features[idx]),\n",
    "            'audio': torch.FloatTensor(self.audio_features[idx]),\n",
    "            'face': torch.FloatTensor(self.face_features[idx]),\n",
    "            'label': torch.FloatTensor([self.labels[idx]])\n",
    "        }\n",
    "\n",
    "class MultimodalFusion(nn.Module):\n",
    "    def __init__(self, text_dim, audio_dim, face_dim, hidden_dim=128):\n",
    "        super(MultimodalFusion, self).__init__()\n",
    "        \n",
    "        # Individual modality encoders\n",
    "        self.text_encoder = nn.Sequential(\n",
    "            nn.Linear(text_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            nn.Linear(audio_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.face_encoder = nn.Sequential(\n",
    "            nn.Linear(face_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for fusion\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4)\n",
    "        \n",
    "        # Final classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, text, audio, face):\n",
    "        # Encode each modality\n",
    "        text_encoded = self.text_encoder(text)\n",
    "        audio_encoded = self.audio_encoder(audio)\n",
    "        face_encoded = self.face_encoder(face)\n",
    "        \n",
    "        # Stack encodings for attention\n",
    "        stacked = torch.stack([text_encoded, audio_encoded, face_encoded])\n",
    "        \n",
    "        # Apply self-attention\n",
    "        attended, _ = self.attention(stacked, stacked, stacked)\n",
    "        \n",
    "        # Concatenate attended features\n",
    "        fused = torch.cat([\n",
    "            attended[0], attended[1], attended[2]\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.classifier(fused)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            text = batch['text'].to(device)\n",
    "            audio = batch['audio'].to(device)\n",
    "            face = batch['face'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(text, audio, face)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                text = batch['text'].to(device)\n",
    "                audio = batch['audio'].to(device)\n",
    "                face = batch['face'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(text, audio, face)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

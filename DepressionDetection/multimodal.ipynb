{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Depression Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:21:18.440199Z",
     "start_time": "2025-07-09T15:21:16.822632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/karlo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/karlo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/karlo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.audio_rnn import AudioRNN\n",
    "from models.face_strnn import FaceSTRNN\n",
    "from models.multimodal_fusion import MultimodalFusion\n",
    "from preprocessing.loader_audio import AudioLoader\n",
    "from preprocessing.loader_face import FaceLoader\n",
    "from preprocessing.loader_results import ResultsLoader\n",
    "from preprocessing.loader_text import TextLoader\n",
    "from training.trainer_multimodal_fusion import MultimodalFusionTrainer\n",
    "\n",
    "# Constants\n",
    "RANDOM_STATE = 42\n",
    "DATA_PERCENTAGE = 0.02  # Percentage of total data to use\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 50\n",
    "FIGURE_SIZE = (15, 8)\n",
    "\n",
    "# Hyperparameter grid for model tuning\n",
    "PARAM_GRID = {\n",
    "    'learning_rate': [0.001, 0.0001],\n",
    "    'weight_decay': [0.01, 0.001],\n",
    "    'dropout': [0.2, 0.3]\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:21:46.241566Z",
     "start_time": "2025-07-09T15:21:18.441526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Data:\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                       TRANSCRIPT_text\nID                                                    \n386  synch introv4confirmation hi im ellie thanks c...\n391  sync introv4confirmation hi im ellie thanks co...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TRANSCRIPT_text</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>386</th>\n      <td>synch introv4confirmation hi im ellie thanks c...</td>\n    </tr>\n    <tr>\n      <th>391</th>\n      <td>sync introv4confirmation hi im ellie thanks co...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Audio Data:\n"
     ]
    },
    {
     "data": {
      "text/plain": "                     FORMANT_F2  FORMANT_F3  FORMANT_F1  FORMANT_F4  \\\nID  TIMESTAMP                                                         \n386 0 days 00:00:00  1571.05937  2554.39041  616.206600   3445.9205   \n    0 days 00:00:10  1726.74306  2604.00720  719.887378   3483.4309   \n    0 days 00:00:20  1657.20697  2569.88200  612.623780   3456.0944   \n    0 days 00:00:30  1657.41013  2599.18730  536.712790   3511.1177   \n    0 days 00:00:40  1633.53475  2605.33350  470.127492   3511.1365   \n\n                     FORMANT_F5  COVAREP_F0  COVAREP_H1H2  COVAREP_HMPDM_12  \\\nID  TIMESTAMP                                                                 \n386 0 days 00:00:00   4352.0802  245.137862      3.553693         -0.042045   \n    0 days 00:00:10   4403.4083  250.681000      4.400846          0.161784   \n    0 days 00:00:20   4344.0295  248.715000      3.023285          0.014002   \n    0 days 00:00:30   4367.7521  234.705000      3.291128          0.018634   \n    0 days 00:00:40   4338.1031  220.724000      4.272259          0.240675   \n\n                     COVAREP_MCEP_0  COVAREP_HMPDM_19  \nID  TIMESTAMP                                          \n386 0 days 00:00:00      -10.863790          0.203370  \n    0 days 00:00:10      -11.367054          0.138979  \n    0 days 00:00:20      -10.471386          0.145858  \n    0 days 00:00:30      -11.130249          0.219273  \n    0 days 00:00:40      -11.218786          0.339719  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>FORMANT_F2</th>\n      <th>FORMANT_F3</th>\n      <th>FORMANT_F1</th>\n      <th>FORMANT_F4</th>\n      <th>FORMANT_F5</th>\n      <th>COVAREP_F0</th>\n      <th>COVAREP_H1H2</th>\n      <th>COVAREP_HMPDM_12</th>\n      <th>COVAREP_MCEP_0</th>\n      <th>COVAREP_HMPDM_19</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th>TIMESTAMP</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">386</th>\n      <th>0 days 00:00:00</th>\n      <td>1571.05937</td>\n      <td>2554.39041</td>\n      <td>616.206600</td>\n      <td>3445.9205</td>\n      <td>4352.0802</td>\n      <td>245.137862</td>\n      <td>3.553693</td>\n      <td>-0.042045</td>\n      <td>-10.863790</td>\n      <td>0.203370</td>\n    </tr>\n    <tr>\n      <th>0 days 00:00:10</th>\n      <td>1726.74306</td>\n      <td>2604.00720</td>\n      <td>719.887378</td>\n      <td>3483.4309</td>\n      <td>4403.4083</td>\n      <td>250.681000</td>\n      <td>4.400846</td>\n      <td>0.161784</td>\n      <td>-11.367054</td>\n      <td>0.138979</td>\n    </tr>\n    <tr>\n      <th>0 days 00:00:20</th>\n      <td>1657.20697</td>\n      <td>2569.88200</td>\n      <td>612.623780</td>\n      <td>3456.0944</td>\n      <td>4344.0295</td>\n      <td>248.715000</td>\n      <td>3.023285</td>\n      <td>0.014002</td>\n      <td>-10.471386</td>\n      <td>0.145858</td>\n    </tr>\n    <tr>\n      <th>0 days 00:00:30</th>\n      <td>1657.41013</td>\n      <td>2599.18730</td>\n      <td>536.712790</td>\n      <td>3511.1177</td>\n      <td>4367.7521</td>\n      <td>234.705000</td>\n      <td>3.291128</td>\n      <td>0.018634</td>\n      <td>-11.130249</td>\n      <td>0.219273</td>\n    </tr>\n    <tr>\n      <th>0 days 00:00:40</th>\n      <td>1633.53475</td>\n      <td>2605.33350</td>\n      <td>470.127492</td>\n      <td>3511.1365</td>\n      <td>4338.1031</td>\n      <td>220.724000</td>\n      <td>4.272259</td>\n      <td>0.240675</td>\n      <td>-11.218786</td>\n      <td>0.339719</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Face Data:\n"
     ]
    },
    {
     "data": {
      "text/plain": "                     CLNFAUs_frame  CLNFAUs_AU12_c  CLNFAUs_AU04_c  \\\nID  TIMESTAMP                                                        \n386 0 days 00:00:00          150.5      -28.936667      -28.793333   \n    0 days 00:00:10          450.5      -14.650000      -14.263333   \n    0 days 00:00:20          750.5        0.530000        0.276667   \n    0 days 00:00:30         1050.5        0.013333        0.613333   \n    0 days 00:00:40         1350.5        0.433333        0.450000   \n\n                     CLNFAUs_AU28_c  CLNFAUs_AU15_c  CLNFAUs_AU23_c  \\\nID  TIMESTAMP                                                         \n386 0 days 00:00:00      -28.820000      -28.816667      -28.300000   \n    0 days 00:00:10      -14.423333      -14.596667      -13.833333   \n    0 days 00:00:20        0.196667        0.063333        0.993333   \n    0 days 00:00:30        0.630000        0.113333        1.000000   \n    0 days 00:00:40        0.796667        0.023333        1.000000   \n\n                     CLNFAUs_AU45_c  CLNFAUs_AU04_r  CLNFAUs_AU01_r  \\\nID  TIMESTAMP                                                         \n386 0 days 00:00:00      -28.793333        0.009116        0.125502   \n    0 days 00:00:10      -14.413333        0.004848        0.025667   \n    0 days 00:00:20        0.176667        0.005845        0.183318   \n    0 days 00:00:30        0.100000        0.001677        0.022389   \n    0 days 00:00:40        0.070000        0.000000        0.003746   \n\n                     CLNFAUs_AU25_r  ...  CLNFgaze_confidence  CLNFpose_frame  \\\nID  TIMESTAMP                        ...                                        \n386 0 days 00:00:00        0.053306  ...             0.843113           150.5   \n    0 days 00:00:10        0.012548  ...             0.862236           450.5   \n    0 days 00:00:20        0.197998  ...             0.976928           750.5   \n    0 days 00:00:30        0.005373  ...             0.980110          1050.5   \n    0 days 00:00:40        0.017477  ...             0.981177          1350.5   \n\n                     CLNFpose_Tz  CLNFpose_Ty  CLNFpose_Tx  CLNFpose_success  \\\nID  TIMESTAMP                                                                  \n386 0 days 00:00:00   526.019457    75.269915    61.237442          0.823333   \n    0 days 00:00:10   554.511130    40.483648    64.456001          0.853333   \n    0 days 00:00:20   529.757020    34.315661    44.287600          1.000000   \n    0 days 00:00:30   528.234877    35.625105    45.062875          1.000000   \n    0 days 00:00:40   531.534767    33.735612    46.428730          1.000000   \n\n                     CLNFpose_Rx  CLNFpose_confidence  CLNFpose_Rz  \\\nID  TIMESTAMP                                                        \n386 0 days 00:00:00     0.273747             0.843113     0.056375   \n    0 days 00:00:10     0.013761             0.862236    -0.064077   \n    0 days 00:00:20     0.065726             0.976928     0.016850   \n    0 days 00:00:30     0.049616             0.980110     0.003288   \n    0 days 00:00:40     0.046129             0.981177     0.004121   \n\n                     CLNFpose_Ry  \nID  TIMESTAMP                     \n386 0 days 00:00:00    -0.056813  \n    0 days 00:00:10    -0.057492  \n    0 days 00:00:20     0.017550  \n    0 days 00:00:30     0.019775  \n    0 days 00:00:40     0.018015  \n\n[5 rows x 29 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>CLNFAUs_frame</th>\n      <th>CLNFAUs_AU12_c</th>\n      <th>CLNFAUs_AU04_c</th>\n      <th>CLNFAUs_AU28_c</th>\n      <th>CLNFAUs_AU15_c</th>\n      <th>CLNFAUs_AU23_c</th>\n      <th>CLNFAUs_AU45_c</th>\n      <th>CLNFAUs_AU04_r</th>\n      <th>CLNFAUs_AU01_r</th>\n      <th>CLNFAUs_AU25_r</th>\n      <th>...</th>\n      <th>CLNFgaze_confidence</th>\n      <th>CLNFpose_frame</th>\n      <th>CLNFpose_Tz</th>\n      <th>CLNFpose_Ty</th>\n      <th>CLNFpose_Tx</th>\n      <th>CLNFpose_success</th>\n      <th>CLNFpose_Rx</th>\n      <th>CLNFpose_confidence</th>\n      <th>CLNFpose_Rz</th>\n      <th>CLNFpose_Ry</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th>TIMESTAMP</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">386</th>\n      <th>0 days 00:00:00</th>\n      <td>150.5</td>\n      <td>-28.936667</td>\n      <td>-28.793333</td>\n      <td>-28.820000</td>\n      <td>-28.816667</td>\n      <td>-28.300000</td>\n      <td>-28.793333</td>\n      <td>0.009116</td>\n      <td>0.125502</td>\n      <td>0.053306</td>\n      <td>...</td>\n      <td>0.843113</td>\n      <td>150.5</td>\n      <td>526.019457</td>\n      <td>75.269915</td>\n      <td>61.237442</td>\n      <td>0.823333</td>\n      <td>0.273747</td>\n      <td>0.843113</td>\n      <td>0.056375</td>\n      <td>-0.056813</td>\n    </tr>\n    <tr>\n      <th>0 days 00:00:10</th>\n      <td>450.5</td>\n      <td>-14.650000</td>\n      <td>-14.263333</td>\n      <td>-14.423333</td>\n      <td>-14.596667</td>\n      <td>-13.833333</td>\n      <td>-14.413333</td>\n      <td>0.004848</td>\n      <td>0.025667</td>\n      <td>0.012548</td>\n      <td>...</td>\n      <td>0.862236</td>\n      <td>450.5</td>\n      <td>554.511130</td>\n      <td>40.483648</td>\n      <td>64.456001</td>\n      <td>0.853333</td>\n      <td>0.013761</td>\n      <td>0.862236</td>\n      <td>-0.064077</td>\n      <td>-0.057492</td>\n    </tr>\n    <tr>\n      <th>0 days 00:00:20</th>\n      <td>750.5</td>\n      <td>0.530000</td>\n      <td>0.276667</td>\n      <td>0.196667</td>\n      <td>0.063333</td>\n      <td>0.993333</td>\n      <td>0.176667</td>\n      <td>0.005845</td>\n      <td>0.183318</td>\n      <td>0.197998</td>\n      <td>...</td>\n      <td>0.976928</td>\n      <td>750.5</td>\n      <td>529.757020</td>\n      <td>34.315661</td>\n      <td>44.287600</td>\n      <td>1.000000</td>\n      <td>0.065726</td>\n      <td>0.976928</td>\n      <td>0.016850</td>\n      <td>0.017550</td>\n    </tr>\n    <tr>\n      <th>0 days 00:00:30</th>\n      <td>1050.5</td>\n      <td>0.013333</td>\n      <td>0.613333</td>\n      <td>0.630000</td>\n      <td>0.113333</td>\n      <td>1.000000</td>\n      <td>0.100000</td>\n      <td>0.001677</td>\n      <td>0.022389</td>\n      <td>0.005373</td>\n      <td>...</td>\n      <td>0.980110</td>\n      <td>1050.5</td>\n      <td>528.234877</td>\n      <td>35.625105</td>\n      <td>45.062875</td>\n      <td>1.000000</td>\n      <td>0.049616</td>\n      <td>0.980110</td>\n      <td>0.003288</td>\n      <td>0.019775</td>\n    </tr>\n    <tr>\n      <th>0 days 00:00:40</th>\n      <td>1350.5</td>\n      <td>0.433333</td>\n      <td>0.450000</td>\n      <td>0.796667</td>\n      <td>0.023333</td>\n      <td>1.000000</td>\n      <td>0.070000</td>\n      <td>0.000000</td>\n      <td>0.003746</td>\n      <td>0.017477</td>\n      <td>...</td>\n      <td>0.981177</td>\n      <td>1350.5</td>\n      <td>531.534767</td>\n      <td>33.735612</td>\n      <td>46.428730</td>\n      <td>1.000000</td>\n      <td>0.046129</td>\n      <td>0.981177</td>\n      <td>0.004121</td>\n      <td>0.018015</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 29 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results Data:\n"
     ]
    },
    {
     "data": {
      "text/plain": "     PHQ_Binary\nID             \n386           1\n391           0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PHQ_Binary</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>386</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>391</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Any\n",
    "from utils.pca_utils import load_and_transform_pca\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "    percentage: float = DATA_PERCENTAGE, random_state: int = RANDOM_STATE\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    # Initialize loaders\n",
    "    results_loader = ResultsLoader()\n",
    "    text_loader = TextLoader()\n",
    "    audio_loader = AudioLoader()\n",
    "    face_loader = FaceLoader()\n",
    "\n",
    "    # Load data\n",
    "    df_result = results_loader.get_data(\n",
    "        percentage=percentage, random_state=random_state\n",
    "    )\n",
    "    df_text = text_loader.get_data(percentage=percentage, random_state=random_state)\n",
    "    df_audio = audio_loader.get_data(\n",
    "        percentage=percentage, random_state=random_state, ds_freq=\"10s\", rw_size=\"10s\"\n",
    "    )\n",
    "    df_face = face_loader.get_data(\n",
    "        percentage=percentage, random_state=random_state, ds_freq=\"10s\", rw_size=\"10s\"\n",
    "    )\n",
    "\n",
    "    # text features preprocessing\n",
    "    # ...\n",
    "\n",
    "    # Audio features PCA\n",
    "    df_audio_pca = load_and_transform_pca(df_audio, [\"models/pca_audio.pkl\"])\n",
    "\n",
    "    # Face features PCA\n",
    "    df_face_pca = load_and_transform_pca(\n",
    "        df_face,\n",
    "        [\n",
    "            \"models/pca_face_action_units.pkl\",\n",
    "            \"models/pca_face_gaze.pkl\",\n",
    "            \"models/pca_face_pose.pkl\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return df_text, df_audio_pca, df_face_pca, df_result\n",
    "\n",
    "\n",
    "def load_models() -> Tuple[Any, nn.Module, nn.Module, StandardScaler, StandardScaler]:\n",
    "    # Load individual models and their preprocessors.\n",
    "    # Load text model\n",
    "    text_model = joblib.load(\"text_model.joblib\")\n",
    "\n",
    "    import training.trainer as train\n",
    "\n",
    "    # Load audio and face models\n",
    "    audio_model, audio_scaler = train.load_model(AudioRNN, \"audio_model.pth\", DEVICE)\n",
    "\n",
    "    face_model, face_scaler = train.load_model(FaceSTRNN, \"face_model.pth\",DEVICE)\n",
    "\n",
    "    return text_model, audio_model, face_model, audio_scaler, face_scaler\n",
    "\n",
    "\n",
    "# Load and prepare data\n",
    "df_text, df_audio, df_face, df_result = prepare_data()\n",
    "\n",
    "# Load models\n",
    "# text_model, audio_model, face_model, audio_scaler, face_scaler = load_models()\n",
    "\n",
    "# Display data overview\n",
    "print(\"Text Data:\")\n",
    "display(df_text.head())\n",
    "\n",
    "print(\"\\nAudio Data:\")\n",
    "display(df_audio.head())\n",
    "\n",
    "print(\"\\nFace Data:\")\n",
    "display(df_face.head())\n",
    "\n",
    "print(\"\\nResults Data:\")\n",
    "display(df_result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:21:46.286645Z",
     "start_time": "2025-07-09T15:21:46.242953Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function aligns and merges the three modalities (text, audio, face) by a common set of keys (ID and time window).\n",
    "# Audio and face are both time series data, so they are expected to have features extracted per time window (e.g., every 10s).\n",
    "# Text is non-time series, but for fusion, we align each text sample to the same time window as audio/face (e.g., by transcript segment or by aggregating text features per window).\n",
    "# The merge ensures that each row in the final dataset corresponds to a single sample with all three modalities for the same subject and time window.\n",
    "# After merging, the function performs a stratified train/val/test split, so that all splits are aligned across modalities.\n",
    "# This ensures that each sample in the split contains the correct text, audio, and face features for the same instance.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def prepare_aligned_data_splits(\n",
    "    df_text: pd.DataFrame,\n",
    "    df_audio: pd.DataFrame,\n",
    "    df_face: pd.DataFrame,\n",
    "    df_result: pd.DataFrame,\n",
    "    test_size: float = 0.2,\n",
    "    val_size: float = 0.1,\n",
    "    random_state: int = RANDOM_STATE\n",
    "):\n",
    "    # Merge on ID and time window (adjust 'window' to your actual time window column if needed)\n",
    "    merge_keys = ['ID', 'window'] if 'window' in df_audio.columns else ['ID']\n",
    "    df = df_result.copy()\n",
    "    df_all = df_text.merge(df_audio, on=merge_keys, suffixes=('_text', '_audio'))\n",
    "    df_all = df_all.merge(df_face, on=merge_keys, suffixes=('', '_face'))\n",
    "    df_all = df_all.merge(df_result, on='ID')\n",
    "\n",
    "    # Drop rows with missing values (optional, or handle differently)\n",
    "    df_all = df_all.dropna()\n",
    "\n",
    "    # Prepare features and target\n",
    "    text_features = df_all['TRANSCRIPT_text']  # or your text feature columns\n",
    "    audio_features = df_all[[col for col in df_audio.columns if col not in merge_keys]]\n",
    "    face_features = df_all[[col for col in df_face.columns if col not in merge_keys]]\n",
    "    y = df_all['PHQ_Binary']\n",
    "\n",
    "    # Train/val/test split (stratified if possible)\n",
    "    X = pd.DataFrame({\n",
    "        'text': text_features,\n",
    "        'audio': list(audio_features.values),\n",
    "        'face': list(face_features.values)\n",
    "    })\n",
    "    y = y.reset_index(drop=True)\n",
    "\n",
    "    # First split into train+val and test\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    # Then split train+val into train and val\n",
    "    val_relative_size = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_trainval, y_trainval, test_size=val_relative_size, random_state=random_state, stratify=y_trainval\n",
    "    )\n",
    "\n",
    "    # Return splits as tuples of (text, audio, face, y)\n",
    "    def unpack_split(X_split, y_split):\n",
    "        return {\n",
    "            'text': list(X_split['text']),\n",
    "            'audio': np.stack(X_split['audio']),\n",
    "            'face': np.stack(X_split['face']),\n",
    "            'label': y_split.values\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        'train': unpack_split(X_train, y_train),\n",
    "        'val': unpack_split(X_val, y_val),\n",
    "        'test': unpack_split(X_test, y_test)\n",
    "    }\n",
    "splits = prepare_aligned_data_splits(df_text, df_audio, df_face, df_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:21:58.706871Z",
     "start_time": "2025-07-09T15:21:46.289182Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]/Users/karlo/College/Diplomski/Code/multimodal_depression_detection/DepressionDetection/.venv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "\n",
      "Training:   0%|          | 0/341 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (32x2 and 128x256)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 112\u001B[0m\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m best_params, results\n\u001B[1;32m    111\u001B[0m \u001B[38;5;66;03m# Train model with grid search\u001B[39;00m\n\u001B[0;32m--> 112\u001B[0m best_params, results \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model_with_grid_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mPARAM_GRID\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;66;03m# Print best parameters\u001B[39;00m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mBest parameters:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[4], line 88\u001B[0m, in \u001B[0;36mtrain_model_with_grid_search\u001B[0;34m(splits, param_grid, n_epochs)\u001B[0m\n\u001B[1;32m     79\u001B[0m trainer \u001B[38;5;241m=\u001B[39m MultimodalFusionTrainer(\n\u001B[1;32m     80\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     81\u001B[0m     criterion\u001B[38;5;241m=\u001B[39mcriterion,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     84\u001B[0m     device\u001B[38;5;241m=\u001B[39mDEVICE\n\u001B[1;32m     85\u001B[0m )\n\u001B[1;32m     87\u001B[0m \u001B[38;5;66;03m# Train model\u001B[39;00m\n\u001B[0;32m---> 88\u001B[0m train_losses, val_losses \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     91\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_epochs\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;66;03m# Record results\u001B[39;00m\n\u001B[1;32m     95\u001B[0m final_val_loss \u001B[38;5;241m=\u001B[39m val_losses[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/College/Diplomski/Code/multimodal_depression_detection/DepressionDetection/training/trainer_multimodal_fusion.py:72\u001B[0m, in \u001B[0;36mMultimodalFusionTrainer.train\u001B[0;34m(self, train_loader, val_loader, n_epochs)\u001B[0m\n\u001B[1;32m     68\u001B[0m val_losses \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_epochs):\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;66;03m# Training\u001B[39;00m\n\u001B[0;32m---> 72\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m     train_losses\u001B[38;5;241m.\u001B[39mappend(train_loss)\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;66;03m# Validation\u001B[39;00m\n",
      "File \u001B[0;32m~/College/Diplomski/Code/multimodal_depression_detection/DepressionDetection/training/trainer_multimodal_fusion.py:29\u001B[0m, in \u001B[0;36mMultimodalFusionTrainer.train_epoch\u001B[0;34m(self, train_loader)\u001B[0m\n\u001B[1;32m     26\u001B[0m batch_y \u001B[38;5;241m=\u001B[39m batch_y\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 29\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_audio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_face\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(outputs, batch_y)\n\u001B[1;32m     31\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/College/Diplomski/Code/multimodal_depression_detection/DepressionDetection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/College/Diplomski/Code/multimodal_depression_detection/DepressionDetection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/College/Diplomski/Code/multimodal_depression_detection/DepressionDetection/models/multimodal_fusion.py:74\u001B[0m, in \u001B[0;36mMultimodalFusion.forward\u001B[0;34m(self, text_input, audio_input, face_input)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     73\u001B[0m     audio_output \u001B[38;5;241m=\u001B[39m audio_result\n\u001B[0;32m---> 74\u001B[0m audio_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maudio_projection\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     76\u001B[0m face_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mface_model(face_input)\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(face_result, \u001B[38;5;28mtuple\u001B[39m):\n",
      "File \u001B[0;32m~/College/Diplomski/Code/multimodal_depression_detection/DepressionDetection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/College/Diplomski/Code/multimodal_depression_detection/DepressionDetection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/College/Diplomski/Code/multimodal_depression_detection/DepressionDetection/.venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: linear(): input and weight.T shapes cannot be multiplied (32x2 and 128x256)"
     ]
    }
   ],
   "source": [
    "def create_data_loaders(\n",
    "    splits: dict,\n",
    "    text_vectorizer,\n",
    "    batch_size: int = BATCH_SIZE\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    def to_tensor(arr, dtype=torch.float32):\n",
    "        return torch.tensor(np.array(arr), dtype=dtype)\n",
    "\n",
    "    # Convert to tensors\n",
    "    # The textual data is transformed into a numeric format using the first step from the pipeline\n",
    "    X_train_text = to_tensor(text_vectorizer.transform(splits['train']['text']).toarray())\n",
    "    X_train_audio = to_tensor(splits['train']['audio'])\n",
    "    X_train_face = to_tensor(splits['train']['face'])\n",
    "    y_train = torch.tensor(splits['train']['label'], dtype=torch.long)\n",
    "\n",
    "    X_val_text = to_tensor(text_vectorizer.transform(splits['val']['text']).toarray())\n",
    "    X_val_audio = to_tensor(splits['val']['audio'])\n",
    "    X_val_face = to_tensor(splits['val']['face'])\n",
    "    y_val = torch.tensor(splits['val']['label'], dtype=torch.long)\n",
    "\n",
    "    X_test_text = to_tensor(text_vectorizer.transform(splits['test']['text']).toarray())\n",
    "    X_test_audio = to_tensor(splits['test']['audio'])\n",
    "    X_test_face = to_tensor(splits['test']['face'])\n",
    "    y_test = torch.tensor(splits['test']['label'], dtype=torch.long)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X_train_text, X_train_audio, X_train_face, y_train)\n",
    "    val_dataset = TensorDataset(X_val_text, X_val_audio, X_val_face, y_val)\n",
    "    test_dataset = TensorDataset(X_test_text, X_test_audio, X_test_face, y_test)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def train_model_with_grid_search(splits: dict, param_grid: Dict, n_epochs: int = N_EPOCHS) -> Tuple[Dict, List[Dict]]:\n",
    "    #Perform grid search to find optimal hyperparameters.\n",
    "    # Load individual models\n",
    "    text_model, audio_model, face_model, audio_scaler, face_scaler = load_models()\n",
    "    text_vectorizer = text_model.named_steps[\"tfidf\"]\n",
    "    text_feature_dim = len(text_vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, _ = create_data_loaders(splits, text_vectorizer, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    results = []\n",
    "\n",
    "    # Grid search\n",
    "    for params in tqdm(ParameterGrid(param_grid)):\n",
    "        # Create multimodal model\n",
    "        model = MultimodalFusion(\n",
    "            text_feature_dim,\n",
    "            audio_model,\n",
    "            face_model\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Training setup\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=params['learning_rate'],\n",
    "            weight_decay=params['weight_decay']\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.1,\n",
    "            patience=3,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = MultimodalFusionTrainer(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        train_losses, val_losses = trainer.train(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            n_epochs=n_epochs\n",
    "        )\n",
    "\n",
    "        # Record results\n",
    "        final_val_loss = val_losses[-1]\n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'final_val_loss': final_val_loss,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses\n",
    "        })\n",
    "\n",
    "        # Update best parameters\n",
    "        if final_val_loss < best_val_loss:\n",
    "            best_val_loss = final_val_loss\n",
    "            best_params = params\n",
    "\n",
    "    return best_params, results\n",
    "\n",
    "\n",
    "# Train model with grid search\n",
    "best_params, results = train_model_with_grid_search(splits, PARAM_GRID)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"\\nBest parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "# print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Plot training curves for best model\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "best_result = min(results, key=lambda x: x['final_val_loss'])\n",
    "plt.plot(best_result['train_losses'], label='Training Loss')\n",
    "plt.plot(best_result['val_losses'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss (Best Model)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader, device: torch.device) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    #Evaluate the model on the test set.\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "\n",
    "# Create test loader\n",
    "_, _, test_loader = create_data_loaders(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "# Load individual models\n",
    "text_model, audio_model, face_model, audio_scaler, face_scaler = load_models()\n",
    "\n",
    "# Initialize best model\n",
    "best_model = MultimodalFusion(\n",
    "    text_model,\n",
    "    audio_model,\n",
    "    face_model\n",
    ").to(DEVICE)\n",
    "\n",
    "# Evaluate model\n",
    "y_true, y_pred = evaluate_model(best_model, test_loader, DEVICE)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(best_model.state_dict(), 'multimodal_model.pth')\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

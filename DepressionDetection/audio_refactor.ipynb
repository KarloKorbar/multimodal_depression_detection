{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio-based Depression Detection Model\n",
    "\n",
    "This notebook implements a deep learning model for depression detection using audio features.\n",
    "The model uses an LSTM architecture with attention mechanisms to process temporal audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T13:47:27.003331Z",
     "start_time": "2025-04-16T13:47:25.630194Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ResultsLoader' from 'preprocessing.loader' (/Users/karlo/College/Diplomski/Code/multimodal_depression_detection/DepressionDetection/preprocessing/loader.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Local imports\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ResultsLoader, AudioLoader\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01maudio_rnn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AudioRNN\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtraining\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AudioRNNTrainer\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'ResultsLoader' from 'preprocessing.loader' (/Users/karlo/College/Diplomski/Code/multimodal_depression_detection/DepressionDetection/preprocessing/loader.py)"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local imports\n",
    "from preprocessing.loader_results import ResultsLoader\n",
    "from preprocessing.loader_audio import AudioLoader\n",
    "from models.audio_rnn import AudioRNN\n",
    "from utils.trainer_audio_rnn import AudioRNNTrainer\n",
    "\n",
    "# Constants\n",
    "RANDOM_STATE = 42\n",
    "DATA_PERCENTAGE = 0.02  # Percentage of total data to use\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 50\n",
    "FIGURE_SIZE = (15, 8)\n",
    "\n",
    "# Hyperparameter grid for model tuning\n",
    "PARAM_GRID = {\n",
    "    'hidden_size': [64, 128],\n",
    "    'num_layers': [1, 2],\n",
    "    'dropout': [0.2, 0.3],\n",
    "    'learning_rate': [0.001, 0.0001],\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Load and preprocess the audio data and depression labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(percentage: float = DATA_PERCENTAGE, random_state: int = RANDOM_STATE) -> Tuple[\n",
    "    pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load and prepare the audio data and depression labels.\n",
    "    \n",
    "    Args:\n",
    "        percentage: Percentage of total data to use\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing audio features DataFrame and results DataFrame\n",
    "    \"\"\"\n",
    "    # Initialize loaders\n",
    "    results_loader = ResultsLoader()\n",
    "    audio_loader = AudioLoader()\n",
    "\n",
    "    # Load data\n",
    "    df_result = results_loader.get_data(percentage=percentage, random_state=random_state)\n",
    "    df_audio = audio_loader.get_data(\n",
    "        percentage=percentage,\n",
    "        random_state=random_state,\n",
    "        ds_freq=\"10s\",\n",
    "        rw_size=\"10s\"\n",
    "    )\n",
    "\n",
    "    return df_audio, df_result\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df_audio, df_result = load_data()\n",
    "\n",
    "# Display data overview\n",
    "print(\"Audio Features Overview:\")\n",
    "display(df_audio.head())\n",
    "print(\"\\nDepression Labels Overview:\")\n",
    "display(df_result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Analyze the distribution of audio features and their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distributions(df: pd.DataFrame, features: List[str], fig_size: Tuple[int, int] = FIGURE_SIZE):\n",
    "    \"\"\"Plot distributions of specified features.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the features\n",
    "        features: List of feature names to plot\n",
    "        fig_size: Figure size tuple (width, height)\n",
    "    \"\"\"\n",
    "    # for plotting distributions of key audio features (provided as a param for flexibility)\n",
    "\n",
    "    n_features = len(features)\n",
    "    n_cols = min(2, n_features)\n",
    "    n_rows = (n_features + 1) // 2\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=fig_size)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for idx, feature in enumerate(features):\n",
    "        sns.histplot(data=df[feature], ax=axes[idx])\n",
    "        axes[idx].set_title(f'Distribution of {feature}')\n",
    "        axes[idx].set_xlabel(feature)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_energy_distribution(df: pd.DataFrame, fig_size: Tuple[int, int] = FIGURE_SIZE):\n",
    "    \"\"\"Plot energy distribution over time for each subject.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing audio features\n",
    "        fig_size: Figure size tuple (width, height)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=fig_size)\n",
    "    for id in df.index.get_level_values('ID').unique():\n",
    "        plt.plot(df.loc[id]['AUDIO_AMPLITUDE'],\n",
    "                 alpha=0.5,\n",
    "                 label=f'Subject {id}')\n",
    "    plt.title('Energy Distribution Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_formant_frequencies(df: pd.DataFrame, fig_size: Tuple[int, int] = FIGURE_SIZE):\n",
    "    \"\"\"Plot average formant frequencies by subject.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing audio features\n",
    "        fig_size: Figure size tuple (width, height)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=fig_size)\n",
    "    formant_features = ['FORMANT_F1', 'FORMANT_F2', 'FORMANT_F3']\n",
    "\n",
    "    for id in df.index.get_level_values('ID').unique():\n",
    "        subject_data = df.loc[id][formant_features]\n",
    "        plt.plot(subject_data.mean(), 'o-', label=f'Subject {id}')\n",
    "\n",
    "    plt.title('Average Formant Frequencies by Subject')\n",
    "    plt.xlabel('Formant Number')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.xticks([0, 1, 2], ['F1', 'F2', 'F3'])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Basic statistics\n",
    "print(\"Basic statistics for audio features:\")\n",
    "print(df_audio.describe())\n",
    "\n",
    "# Plot distributions of key audio features\n",
    "audio_features = ['AUDIO_AMPLITUDE', 'FORMANT_F1', 'FORMANT_F2', 'FORMANT_F3']\n",
    "plot_feature_distributions(df_audio, audio_features)\n",
    "\n",
    "# Energy distribution over time\n",
    "plot_energy_distribution(df_audio)\n",
    "\n",
    "# Formant frequencies analysis\n",
    "plot_formant_frequencies(df_audio)\n",
    "\n",
    "# COVAREP features analysis\n",
    "covarep_cols = [col for col in df_audio.columns if 'COVAREP' in col]\n",
    "if covarep_cols:\n",
    "    plt.figure(figsize=FIGURE_SIZE)\n",
    "    sns.heatmap(df_audio[covarep_cols].corr(),\n",
    "                annot=True,\n",
    "                cmap='coolwarm',\n",
    "                center=0)\n",
    "    plt.title('Correlation between COVAREP features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# TODO: missing speech rate analysis (do i need it?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "Perform PCA to reduce dimensionality and identify key features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def perform_pca(df: pd.DataFrame, n_components: int = 10) -> Tuple[pd.DataFrame, PCA]:\n",
    "    \"\"\"Perform PCA on the input data.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        n_components: Number of principal components to keep\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing PCA-transformed DataFrame and fitted PCA object\n",
    "    \"\"\"\n",
    "    # Select numerical features\n",
    "    X = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # Initialize and fit PCA\n",
    "    pca = PCA(n_components=n_components)  # adjust n_components \n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    # Create DataFrame with PCA results\n",
    "    pca_df = pd.DataFrame(\n",
    "        X_pca,\n",
    "        columns=[f'PC{i + 1}' for i in range(X_pca.shape[1])],\n",
    "        index=X.index\n",
    "    )\n",
    "\n",
    "    return pca_df, pca\n",
    "\n",
    "\n",
    "# Perform PCA\n",
    "pca_df, pca = perform_pca(df_audio)\n",
    "\n",
    "# Plot explained variance ratio\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('PCA Explained Variance Ratio')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot first two principal components\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "for id in pca_df.index.get_level_values('ID').unique():\n",
    "    mask = pca_df.index.get_level_values('ID') == id\n",
    "    plt.scatter(pca_df[mask]['PC1'],\n",
    "                pca_df[mask]['PC2'],\n",
    "                alpha=0.6,\n",
    "                label=f'Subject {id}')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('First Two Principal Components')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print explained variance ratios\n",
    "print(\"\\nExplained variance ratio for each component:\")\n",
    "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"PC{i + 1}: {ratio:.4f}\")\n",
    "\n",
    "# Print Cumulative explained variance ratio\n",
    "print(\"\\nCumulative explained variance ratio:\")\n",
    "print(np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "# TODO: missing get feature importance and plot heatmap of feature importance for the first few components "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "Split the data into training, validation, and test sets while preserving temporal order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_splits(df_audio: pd.DataFrame, df_result: pd.DataFrame) -> Tuple[\n",
    "    np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Prepare data splits for training, validation, and testing.\n",
    "    \n",
    "    Args:\n",
    "        df_audio: DataFrame containing audio features\n",
    "        df_result: DataFrame containing depression labels\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing training, validation, and test data splits\n",
    "    \"\"\"\n",
    "    # Merge audio features with depression labels\n",
    "    df = pd.merge(df_audio, df_result, on='ID')  # TODO: check if this is the same df as the one modified in the PCA\n",
    "\n",
    "    # Prepare features and target\n",
    "    X = df.drop(['PHQ_Binary'], axis=1)\n",
    "    y = df['PHQ_Binary']\n",
    "\n",
    "    # Create time series split\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    # Get split indices while preserving temporal order\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        train_indices.append(train_idx)\n",
    "        test_indices.append(test_idx)\n",
    "\n",
    "    # Use the last fold for final train/test split\n",
    "    X_train = X.iloc[train_indices[-1]]\n",
    "    X_test = X.iloc[test_indices[-1]]\n",
    "    y_train = y.iloc[train_indices[-1]]\n",
    "    y_test = y.iloc[test_indices[-1]]\n",
    "\n",
    "    # Further split training data into train and validation\n",
    "    train_size = int(0.75 * len(X_train))\n",
    "    X_train, X_val = X_train.iloc[:train_size], X_train.iloc[train_size:]\n",
    "    y_train, y_val = y_train.iloc[:train_size], y_train.iloc[train_size:]\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, scaler\n",
    "\n",
    "\n",
    "# Prepare data splits\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler = prepare_data_splits(df_audio, df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Train the audio RNN model with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def create_data_loaders(X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray,\n",
    "                        y_train: np.ndarray, y_val: np.ndarray, y_test: np.ndarray,\n",
    "                        batch_size: int = BATCH_SIZE) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"Create PyTorch DataLoaders for training, validation, and testing.\n",
    "    \n",
    "    Args:\n",
    "        X_train, X_val, X_test: Feature arrays\n",
    "        y_train, y_val, y_test: Label arrays\n",
    "        batch_size: Batch size for training\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing train, validation, and test DataLoaders\n",
    "    \"\"\"\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_train_tensor = torch.LongTensor(y_train.values)\n",
    "    y_val_tensor = torch.LongTensor(y_val.values)\n",
    "    y_test_tensor = torch.LongTensor(y_test.values)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def train_model_with_grid_search(X_train: np.ndarray, y_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray,\n",
    "                                 param_grid: Dict, n_epochs: int = N_EPOCHS) -> Tuple[Dict, List[Dict]]:\n",
    "    \"\"\"Perform grid search to find optimal hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        param_grid: Dictionary of hyperparameters to search\n",
    "        n_epochs: Number of training epochs\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing best parameters and all results\n",
    "    \"\"\"\n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, _ = create_data_loaders(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    results = []\n",
    "\n",
    "    # Grid search\n",
    "    for params in tqdm(ParameterGrid(param_grid)):\n",
    "        # Model initialization\n",
    "        model = AudioRNN(\n",
    "            input_size=X_train.shape[1],\n",
    "            hidden_size=params['hidden_size'],\n",
    "            num_layers=params['num_layers'],\n",
    "            dropout=params['dropout']\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Training setup\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.1,\n",
    "            patience=3,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = AudioRNNTrainer(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        train_losses, val_losses = trainer.train(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            n_epochs=n_epochs\n",
    "        )\n",
    "\n",
    "        # Record results\n",
    "        final_val_loss = val_losses[-1]\n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'final_val_loss': final_val_loss,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses\n",
    "        })\n",
    "\n",
    "        # Update best parameters\n",
    "        if final_val_loss < best_val_loss:\n",
    "            best_val_loss = final_val_loss\n",
    "            best_params = params\n",
    "\n",
    "    return best_params, results\n",
    "\n",
    "\n",
    "# Train model with grid search\n",
    "best_params, results = train_model_with_grid_search(X_train, y_train, X_val, y_val, PARAM_GRID)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"\\nBest parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "# print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Plot training curves for best model\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "best_result = min(results, key=lambda x: x['final_val_loss'])\n",
    "plt.plot(best_result['train_losses'], label='Training Loss')\n",
    "plt.plot(best_result['val_losses'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss (Best Model)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Evaluate the model's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader, device: torch.device) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Evaluate the model on the test set.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        test_loader: DataLoader containing test data\n",
    "        device: Device to run evaluation on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing true labels and predicted labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "\n",
    "# Create test loader\n",
    "_, _, test_loader = create_data_loaders(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "# Initialize best model\n",
    "best_model = AudioRNN(\n",
    "    input_size=X_train.shape[1],\n",
    "    **best_params\n",
    ").to(DEVICE)\n",
    "\n",
    "# Evaluate model\n",
    "y_true, y_pred = evaluate_model(best_model, test_loader, DEVICE)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "\n",
    "Save the trained model and scaler for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.trainer import save_model\n",
    "\n",
    "# Save the model and scaler\n",
    "save_model(best_model, scaler, \"audio_model.pth\")\n",
    "print(\"Model and scaler saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face-based Depression Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.face_strnn import FaceSTRNN\n",
    "from preprocessing.loader_face import FaceLoader\n",
    "from preprocessing.loader_results import ResultsLoader\n",
    "from training.trainer_face_strnn import FaceSTRNNTrainer\n",
    "\n",
    "# Constants\n",
    "RANDOM_STATE = 42\n",
    "DATA_PERCENTAGE = 0.02  # Percentage of total data to use\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 50\n",
    "FIGURE_SIZE = (15, 8)\n",
    "\n",
    "# Hyperparameter grid for model tuning\n",
    "PARAM_GRID = {\n",
    "    'hidden_size': [64, 128, 256],\n",
    "    'num_layers': [1, 2],\n",
    "    'dropout': [0.2, 0.3, 0.4],\n",
    "    'learning_rate': [0.001, 0.0001],\n",
    "    'weight_decay': [0.0001, 0.00001]\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(percentage: float = DATA_PERCENTAGE, random_state: int = RANDOM_STATE) -> Tuple[\n",
    "    pd.DataFrame, pd.DataFrame]:\n",
    "    #Load and prepare the facial features data and depression labels.\n",
    "    # Initialize loaders\n",
    "    results_loader = ResultsLoader()\n",
    "    face_loader = FaceLoader()\n",
    "\n",
    "    # Load data\n",
    "    df_result = results_loader.get_data(percentage=percentage, random_state=random_state)\n",
    "    df_face = face_loader.get_data(\n",
    "        percentage=percentage,\n",
    "        random_state=random_state,\n",
    "        ds_freq=\"10s\",\n",
    "        rw_size=\"10s\"\n",
    "    )\n",
    "\n",
    "    return df_face, df_result\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df_face, df_result = load_data()\n",
    "\n",
    "# Display data overview\n",
    "print(\"Facial Features:\")\n",
    "display(df_face.head())\n",
    "print(\"\\nDepression Labels:\")\n",
    "display(df_result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T19:44:38.813874Z",
     "start_time": "2025-04-16T19:44:38.808243Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1046494009.py, line 91)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[1], line 91\u001B[0;36m\u001B[0m\n\u001B[0;31m    analyze_landmarks(df_face)\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def analyze_landmarks(df: pd.DataFrame) -> None:\n",
    "    #Analyze facial landmarks using PCA and visualization.\n",
    "    # Extract landmark features\n",
    "    landmark_features = [col for col in df.columns if 'CLNFfeatures_' in col]\n",
    "    landmark_data = df[landmark_features]\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=10)\n",
    "    landmark_pca = pca.fit_transform(landmark_data)\n",
    "\n",
    "    # Plot explained variance ratio\n",
    "    plt.figure(figsize=FIGURE_SIZE)\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.title('Explained Variance Ratio of Landmark PCA Components')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot eigenfaces -> double check\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(6):\n",
    "        if i < len(pca.components_):\n",
    "            component = pca.components_[i].reshape(-1, 2)  # Reshape to (68, 2) for x,y coordinates\n",
    "            axes[i].scatter(component[:, 0], component[:, 1], alpha=0.6)\n",
    "            axes[i].set_title(f'Eigenface {i + 1}')\n",
    "            axes[i].set_xlabel('X coordinate')\n",
    "            axes[i].set_ylabel('Y coordinate')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_action_units(df: pd.DataFrame) -> None:\n",
    "    #Analyze facial action units using visualization.\n",
    "    # Extract action unit features\n",
    "    au_features = [col for col in df.columns if 'CLNFAUs_' in col]\n",
    "\n",
    "    # Create violin plots\n",
    "    plt.figure(figsize=FIGURE_SIZE)\n",
    "    sns.violinplot(data=df[au_features])\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Distribution of Action Units')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(df[au_features].corr(), cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation Matrix of Action Units')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_landmark_positions(df: pd.DataFrame) -> None:\n",
    "    #Plot average landmark positions for each subject.\n",
    "    landmark_features = [col for col in df.columns if 'CLNFfeatures_' in col]\n",
    "    mean_landmarks = df.groupby('ID')[landmark_features].mean()\n",
    "\n",
    "    x_coords = mean_landmarks[[col for col in landmark_features if '_x' in col]].values\n",
    "    y_coords = mean_landmarks[[col for col in landmark_features if '_y' in col]].values\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(x_coords, y_coords, alpha=0.5)\n",
    "    plt.title('Average Landmark Positions')\n",
    "    plt.xlabel('X coordinate')\n",
    "    plt.ylabel('Y coordinate')\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Perform EDA\n",
    "analyze_landmarks(df_face)\n",
    "analyze_action_units(df_face)\n",
    "plot_landmark_positions(df_face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pca_utils import save_pca_components\n",
    "\n",
    "def perform_feature_pca(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:\n",
    "    #Perform PCA on different facial feature types.\n",
    "    # Extract different feature types\n",
    "    feature_types = {\n",
    "        'Gaze': [col for col in df.columns if 'CLNFgaze_' in col],\n",
    "        'Action Units': [col for col in df.columns if 'CLNFAUs_' in col],\n",
    "        'Pose': [col for col in df.columns if 'CLNFpose_' in col]\n",
    "    }\n",
    "\n",
    "    # Dictionary to store PCA results\n",
    "    pca_results = {}\n",
    "\n",
    "    # Perform PCA for each feature type\n",
    "    for feature_name, features in feature_types.items():\n",
    "        if not features:\n",
    "            continue\n",
    "\n",
    "        # Extract data and perform PCA\n",
    "        feature_data = df[features]\n",
    "        pca = PCA(n_components=min(10, len(features)))\n",
    "        feature_pca = pca.fit_transform(feature_data)\n",
    "        \n",
    "       # Store results\n",
    "        pca_results[feature_name] = {\n",
    "            'pca': pca,\n",
    "            'transformed': feature_pca\n",
    "        }\n",
    "\n",
    "        # Plot explained variance ratio\n",
    "        plt.figure(figsize=FIGURE_SIZE)\n",
    "        plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "        plt.xlabel('Number of Components')\n",
    "        plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "        plt.title(f'Explained Variance Ratio of {feature_name} PCA Components')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # Print variance explained\n",
    "        print(f\"\\n{feature_name} - Explained variance ratio by component:\")\n",
    "        for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "            print(f\"PC{i + 1}: {ratio:.4f}\")\n",
    "\n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        feature_importance = pd.DataFrame(\n",
    "            pca.components_.T,\n",
    "            columns=[f'PC{i + 1}' for i in range(feature_pca.shape[1])],\n",
    "            index=features\n",
    "        )\n",
    "        sns.heatmap(feature_importance, cmap='coolwarm', center=0)\n",
    "        plt.title(f'Feature Importance in {feature_name} Principal Components')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Create combined PCA features DataFrame\n",
    "    pca_df = pd.DataFrame(index=df.index)\n",
    "    for feature_name, result in pca_results.items():\n",
    "        feature_cols = [f'{feature_name}_PC{i + 1}' for i in range(result['transformed'].shape[1])]\n",
    "        pca_df[feature_cols] = result['transformed']\n",
    "\n",
    "    # Save PCA components\n",
    "    feature_names = pca_df.columns.tolist()\n",
    "    save_pca_components(\n",
    "        pca=pca_df,\n",
    "        feature_names=feature_names,\n",
    "        output_path='models/pca_face.pkl'\n",
    "    )\n",
    "    return pca_df, pca_results\n",
    "\n",
    "\n",
    "# Perform PCA\n",
    "pca_df, pca_results = perform_feature_pca(df_face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_splits(df_face: pd.DataFrame, df_result: pd.DataFrame) -> Tuple[\n",
    "    np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, StandardScaler]:\n",
    "    #Prepare data splits for training, validation, and testing.\n",
    "    # Merge facial features with depression labels\n",
    "    df = pd.merge(df_face, df_result, on='ID')\n",
    "\n",
    "    # Prepare features and target\n",
    "    X = df.drop(['PHQ_Binary'], axis=1)\n",
    "    y = df['PHQ_Binary']\n",
    "\n",
    "    # Create time series split\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    # Get split indices while preserving temporal order\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        train_indices.append(train_idx)\n",
    "        test_indices.append(test_idx)\n",
    "\n",
    "    # Use the last fold for final train/test split\n",
    "    X_train = X.iloc[train_indices[-1]]\n",
    "    X_test = X.iloc[test_indices[-1]]\n",
    "    y_train = y.iloc[train_indices[-1]]\n",
    "    y_test = y.iloc[test_indices[-1]]\n",
    "\n",
    "    # Further split training data into train and validation\n",
    "    train_size = int(0.75 * len(X_train))\n",
    "    X_train, X_val = X_train.iloc[:train_size], X_train.iloc[train_size:]\n",
    "    y_train, y_val = y_train.iloc[:train_size], y_train.iloc[train_size:]\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, scaler\n",
    "\n",
    "\n",
    "# Prepare data splits\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler = prepare_data_splits(pca_df, df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T18:21:35.018875Z",
     "start_time": "2025-04-07T18:21:34.846529Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BATCH_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_data_loaders\u001B[39m(X_train: np\u001B[38;5;241m.\u001B[39mndarray, X_val: np\u001B[38;5;241m.\u001B[39mndarray, X_test: np\u001B[38;5;241m.\u001B[39mndarray,\n\u001B[1;32m      2\u001B[0m                         y_train: np\u001B[38;5;241m.\u001B[39mndarray, y_val: np\u001B[38;5;241m.\u001B[39mndarray, y_test: np\u001B[38;5;241m.\u001B[39mndarray,\n\u001B[0;32m----> 3\u001B[0m                         batch_size: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mBATCH_SIZE\u001B[49m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[DataLoader, DataLoader, DataLoader]:\n\u001B[1;32m      4\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Create PyTorch DataLoaders for training, validation, and testing.\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m    \u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124;03m        Tuple containing train, validation, and test DataLoaders\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;66;03m# Convert data to PyTorch tensors\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'BATCH_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def create_data_loaders(X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray,\n",
    "                        y_train: np.ndarray, y_val: np.ndarray, y_test: np.ndarray,\n",
    "                        batch_size: int = BATCH_SIZE) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    #Create PyTorch DataLoaders for training, validation, and testing.\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_train_tensor = torch.LongTensor(y_train.values)\n",
    "    y_val_tensor = torch.LongTensor(y_val.values)\n",
    "    y_test_tensor = torch.LongTensor(y_test.values)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def train_model_with_grid_search(X_train: np.ndarray, y_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray,\n",
    "                                 param_grid: Dict, n_epochs: int = N_EPOCHS) -> Tuple[Dict, List[Dict]]:\n",
    "    #Perform grid search to find optimal hyperparameters.\n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, _ = create_data_loaders(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    results = []\n",
    "\n",
    "    # Grid search\n",
    "    for params in tqdm(ParameterGrid(param_grid)):\n",
    "        # Model initialization\n",
    "        model = FaceSTRNN(\n",
    "            input_size=X_train.shape[1],\n",
    "            hidden_size=params['hidden_size'],\n",
    "            num_layers=params['num_layers'],\n",
    "            num_classes=2,\n",
    "            dropout=params['dropout']\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Training setup\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=params['learning_rate'],\n",
    "            weight_decay=params['weight_decay']\n",
    "        )\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.1,\n",
    "            patience=3,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        # Initialize trainer \n",
    "        trainer = FaceSTRNNTrainer(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        # Train model \n",
    "        train_losses, val_losses = trainer.train(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            n_epochs=n_epochs\n",
    "        )\n",
    "\n",
    "        # Record results\n",
    "        final_val_loss = val_losses[-1]\n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'final_val_loss': final_val_loss,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses\n",
    "        })\n",
    "\n",
    "        # Update best parameters\n",
    "        if final_val_loss < best_val_loss:\n",
    "            best_val_loss = final_val_loss\n",
    "            best_params = params\n",
    "\n",
    "    return best_params, results\n",
    "\n",
    "\n",
    "# Train model with grid search\n",
    "best_params, results = train_model_with_grid_search(X_train, y_train, X_val, y_val, PARAM_GRID)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"\\nBest parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "# print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Plot training curves for best model\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "best_result = min(results, key=lambda x: x['final_val_loss'])\n",
    "plt.plot(best_result['train_losses'], label='Training Loss')\n",
    "plt.plot(best_result['val_losses'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss (Best Model)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader, device: torch.device) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    #Evaluate the model on the test set.\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs, _, _ = model(X)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "\n",
    "# Create test loader\n",
    "_, _, test_loader = create_data_loaders(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "# Initialize best model\n",
    "best_model = FaceSTRNN(\n",
    "    input_size=X_train.shape[1],\n",
    "    hidden_size=best_params['hidden_size'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    num_classes=2,\n",
    "    dropout=best_params['dropout']\n",
    ").to(DEVICE)\n",
    "\n",
    "# Evaluate model\n",
    "y_true, y_pred = evaluate_model(best_model, test_loader, DEVICE)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and scaler\n",
    "from training.trainer import save_model\n",
    "\n",
    "save_model(best_model, scaler, \"face_model.pth\")\n",
    "print(\"Model and scaler saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
